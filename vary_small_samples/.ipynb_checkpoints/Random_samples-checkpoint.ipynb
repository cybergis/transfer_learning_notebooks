{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_path = 'Covington_data/without_NAIP/nodata_as_0/'\n",
    "name = 'No_NAIP_'\n",
    "# read in training and validation data\n",
    "X_train = np.load(data_path+'train_data.npy')\n",
    "Y_train = np.load(data_path+'train_label.npy')\n",
    "X_Validation = np.load(data_path+'vali_data.npy')\n",
    "Y_Validation = np.load(data_path+'vali_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset has 9 channels:\n",
    "# 0. Curvature\n",
    "# 1. Slope\n",
    "# 2. Openness\n",
    "# 3. DEM\n",
    "# 4. TPI 21\n",
    "# 5. Reflectance (LiDAR intensity)\n",
    "# 6. Geomorphon\n",
    "# 7. TPI 9\n",
    "# 8. TPI 3\n",
    "\n",
    "training_sample_ratio = 0.8\n",
    "sample_size= 10 #10,20,30,40,...,100 \n",
    "training = sample_size * training_sample_ratio\n",
    "validation = sample_size * (1-training_sample_ratio)\n",
    "\n",
    "# # ##############################################\n",
    "# # 100 samples \n",
    "# X_train_new = X_train[0:80,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:80,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:20,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:20,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"100_samples\"\n",
    "# # ##############################################\n",
    "\n",
    "# # ##############################################\n",
    "# # 90 samples \n",
    "# X_train_new = X_train[0:72,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:72,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:18,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:18,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"90_samples_\"\n",
    "# # ##############################################\n",
    "\n",
    "# # ##############################################\n",
    "# # 80 samples \n",
    "# X_train_new = X_train[0:64,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:64,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:16,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:16,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"80_samples_\"\n",
    "# # ##############################################\n",
    "\n",
    "# ##############################################\n",
    "# 70 samples \n",
    "X_train_new = X_train[0:56,:,:,:]\n",
    "print(X_train_new.shape)\n",
    "Y_train = np.load(data_path+'train_label.npy')[0:56,:,:,:]\n",
    "print(Y_train.shape)\n",
    "\n",
    "X_Validation_new = X_Validation[0:14,:,:,:]\n",
    "print(X_Validation_new.shape)\n",
    "Y_Validation = np.load(data_path+'vali_label.npy')[0:14,:,:,:]\n",
    "print(Y_Validation.shape)\n",
    "name += \"70_samples_\"\n",
    "# ##############################################\n",
    "\n",
    "# # ##############################################\n",
    "# # 60 samples \n",
    "# X_train_new = X_train[0:48,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:48,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:12,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:12,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"60_samples_\"\n",
    "# # ##############################################\n",
    "\n",
    "# # ##############################################\n",
    "# # 50 samples \n",
    "# X_train_new = X_train[0:40,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:40,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:10,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:10,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"50_samples_\"\n",
    "# # ##############################################\n",
    "\n",
    "# # ##############################################\n",
    "# # 40 samples \n",
    "# X_train_new = X_train[0:32,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:32,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:8,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:8,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"40_samples_\"\n",
    "# # ##############################################\n",
    "\n",
    "# # ##############################################\n",
    "# # 30 samples \n",
    "# X_train_new = X_train[0:24,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:24,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:6,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:6,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"30_samples_\"\n",
    "# # ##############################################\n",
    "\n",
    "# ##############################################\n",
    "# # 20 samples \n",
    "# X_train_new = X_train[0:16,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:16,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:4,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:4,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"20_samples_\"\n",
    "# ##############################################\n",
    "\n",
    "\n",
    "# ##############################################\n",
    "# # 10 samples \n",
    "# X_train_new = X_train[0:8,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:8,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:2,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:2,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"10_samples_\"\n",
    "# ##############################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/vary_small_samples/samples/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
