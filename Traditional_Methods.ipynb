{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Traditional methods\n",
        "\n",
        "In this study we have evaluate the traditional methods including:\n",
        "- Random Forest\n",
        "- Support Vector Machine\n",
        "- Gradient Boosting Classifier"
      ],
      "metadata": {
        "id": "vh_LPCwLWXS3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJXoHnoFJqqm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import shutil\n",
        "import pickle\n",
        "import warnings\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import randomimport time\n",
        "from PIL import Image as im\n",
        "from sklearn.svm import SVC\n",
        "from datetime import datetime, timezone, timedelta\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import f1_score, precision_score,recall_score, cohen_kappa_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the process"
      ],
      "metadata": {
        "id": "74LC-xZdeRR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specify the smaple folder path\n",
        "input_data = './samples/'\n",
        "model_path = './models/'\n",
        "prediction_path = './predicts/'\n",
        "log_path = './logs/'\n",
        "\n",
        "# Data location\n",
        "# 'covington' 'rowancreek'\n",
        "location = 'covington'\n",
        "\n",
        "# Classifier\n",
        "# \"SVM\", \"RF\", or \"GBC\"\n",
        "classifier = \"SVM\"\n",
        "\n",
        "save_model_path = model_path+location+'_'+classifier"
      ],
      "metadata": {
        "id": "5FzhIKL9eQp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the classifications"
      ],
      "metadata": {
        "id": "KtgRoPu1eJIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classifier_train(train_X, train_Y, save_model_path, classifier):\n",
        "    \"\"\"\n",
        "    Train a classifier model using the provided training data and save the trained model.\n",
        "\n",
        "    Args:\n",
        "        train_X (array-like): Training data features.\n",
        "        train_Y (array-like): Training data labels.\n",
        "        save_model_path (str): Path to save the trained model.\n",
        "        classifier (str): Classifier type: \"SVM\", \"RF\", or \"GBC\".\n",
        "\n",
        "    Returns:\n",
        "        model: Trained classifier model.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    if classifier == \"SVM\":\n",
        "        print(\"_\"*30)\n",
        "        print('[INFO] Training a Support Vector Machine model.')\n",
        "        model = SVC(kernel='linear')\n",
        "    elif classifier == \"RF\":\n",
        "        print(\"_\"*30)\n",
        "        print('[INFO] Training a Random Forest model.')\n",
        "        model = RandomForestClassifier(n_estimators=15, criterion='entropy', max_features='auto',\n",
        "                                       max_depth=8, min_samples_split=3, min_samples_leaf=18, random_state=42)\n",
        "    elif classifier == \"GBC\":\n",
        "        print(\"[INFO] Training a Gradient Boosting model.\")\n",
        "        model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
        "\n",
        "    model.fit(train_X, train_Y)\n",
        "\n",
        "    end_time = time.time()\n",
        "    filename = save_model_path + '.pkl'\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(model, file)\n",
        "\n",
        "    print(\"_\"*30)\n",
        "    print('[INFO] Training complete.')\n",
        "    print('training time: %.4f s' % (end_time - start_time))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "zzXc1B9yW51Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare training data and label"
      ],
      "metadata": {
        "id": "yW2mqmY-enRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data and labels from files\n",
        "X_train = np.load(input_data + location + '/train_data.npy').astype(np.float32)\n",
        "Y_train = np.load(input_data + location + '/train_label.npy').astype(np.float32)\n",
        "\n",
        "# Reshape the training data for further processing\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0] * X_train.shape[1] * X_train.shape[2], X_train.shape[3])\n",
        "X_train_reshaped = X_train_reshaped.flatten()\n",
        "print('Reshaped training data shape:', X_train_reshaped.shape)\n",
        "\n",
        "# Reshape and convert training labels to integers\n",
        "Y_train_reshaped = Y_train.reshape(Y_train.shape[0] * Y_train.shape[1] * Y_train.shape[2], Y_train.shape[3]).astype('int')\n",
        "\n",
        "# Find indices of pixels representing streams\n",
        "stream_idx = np.where(Y_train_reshaped == 1)\n",
        "\n",
        "# Generate an array of indices for all stream pixels\n",
        "stream_px_all = np.arange(len(stream_idx[0]))\n",
        "\n",
        "# Select a random subset of stream pixels\n",
        "n = 10000\n",
        "stream_pxs_idx = np.random.choice(stream_px_all, size=n, replace=False)"
      ],
      "metadata": {
        "id": "Al7Axq5XXV3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_SVM = classifier_train(X_train_reshaped[stream_pxs_idx], Y_train_reshaped[stream_pxs_idx], save_model_path, classifier)"
      ],
      "metadata": {
        "id": "qmZ9xbgYXVvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the trained models to predict the bottom half\n"
      ],
      "metadata": {
        "id": "u5uco8m1XVnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the test data\n",
        "X_test = np.load(input_data+location+'/bottom_half_test_data.npy').astype(np.float32)\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0]*X_test.shape[1]*X_test.shape[2], X_test.shape[3])\n",
        "X_test_reshaped.shape"
      ],
      "metadata": {
        "id": "5c_5WU6AYdtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(test_X, save_model_path, classifier):\n",
        "    \"\"\"\n",
        "    Test the trained model on test data and save the prediction results.\n",
        "\n",
        "    Args:\n",
        "        test_X (array-like): Test data features.\n",
        "        save_model_path (str): Path where the trained model is saved.\n",
        "        classifier (str): Classifier type: \"SVM\", \"RF\", or \"GBC\".\n",
        "    \"\"\"\n",
        "    prediction_path = './predicts/'\n",
        "\n",
        "    # Load the trained model\n",
        "    pkl_filename = save_model_path + classifier + '.pkl'\n",
        "    with open(pkl_filename, 'rb') as file:\n",
        "        model = pickle.load(file)\n",
        "\n",
        "    # Predict using the trained model\n",
        "    pred = model.predict(test_X)\n",
        "\n",
        "    # Reshape the prediction results\n",
        "    predict_reshaped = pred.reshape((test_X.shape[0], test_X.shape[1], test_X.shape[2], 1))\n",
        "\n",
        "    # Save the prediction results\n",
        "    save_path = prediction_path + location + '_' + classifier + '_predict.npy'\n",
        "    np.save(save_path, predict_reshaped)\n",
        "\n",
        "    print(\"Prediction Results saved:\", save_path)\n",
        "\n",
        "    return save_path\n"
      ],
      "metadata": {
        "id": "d6dF48pcX4sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_npy_path = test(X_test_reshaped,save_model_path, classifier)"
      ],
      "metadata": {
        "id": "pJIo2nydZVry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the prediction results"
      ],
      "metadata": {
        "id": "GgtqdZ_hW5nP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_SVM_npy = prediction_path+location+'_'+classifier+'_predict.npy'\n",
        "text_path = prediction_path+'prediction_results.txt'\n",
        "mask_npy = input_data + location + '/bottom_half_test_mask.npy'\n",
        "label_npy = input_data + location + '/bottom_half_test_label.npy'"
      ],
      "metadata": {
        "id": "-k5iE7VgZqvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_prediction_result(location, pred_npy, mask_npy, label_npy, model_path):\n",
        "    \"\"\"\n",
        "    Evaluate the prediction results using various metrics and save the evaluation results to a text file.\n",
        "\n",
        "    Args:\n",
        "        location (str): Location identifier.\n",
        "        pred_npy (str): Path to the prediction numpy file.\n",
        "        mask_npy (str): Path to the mask numpy file.\n",
        "        label_npy (str): Path to the label numpy file.\n",
        "        model_path (str): Path to the model used for prediction.\n",
        "        txt_path (str): Path to save the evaluation results as a text file.\n",
        "    \"\"\"\n",
        "    prediction_npy = np.load(pred_npy)\n",
        "    prediction_mask_npy = np.load(mask_npy)\n",
        "    predition_label_npy = np.load(label_npy)\n",
        "\n",
        "    dim = predition_label_npy.shape\n",
        "    buf = 30\n",
        "    numr = dim[0] // (224 - buf * 2)\n",
        "    numc = dim[1] // (224 - buf * 2)\n",
        "    count = -1\n",
        "    for i in range(numr):\n",
        "        if (location == 'covington' and i == 20):\n",
        "            break\n",
        "\n",
        "        # Concatenate each column to create row ith\n",
        "        numc_con = int(numc/2) - 1 if(location == 'covington') else numc\n",
        "        for j in range(numc_con):\n",
        "            count += 1\n",
        "            temp = prediction_npy[count][buf:-buf, buf:-buf]\n",
        "            if j == 0:\n",
        "                rows = temp\n",
        "            else:\n",
        "                rows = np.concatenate((rows, temp), axis=1)\n",
        "\n",
        "        # Concatenate the row ith to the total prediction\n",
        "        if i == 0:\n",
        "            prediction_map = copy.copy(rows)\n",
        "        else:\n",
        "            prediction_map = np.concatenate((prediction_map, rows), axis=0)\n",
        "\n",
        "    prediction_map = prediction_map[:, :, 0]\n",
        "\n",
        "    # Load mask and ground truth data\n",
        "    mask = prediction_mask_npy[:prediction_map.shape[0], :prediction_map.shape[1]]\n",
        "    [lr, lc] = np.where(mask == 1)\n",
        "    groundtruth = predition_label_npy[:prediction_map.shape[0], :prediction_map.shape[1]]\n",
        "    groundtruthlist = predition_label_npy[:prediction_map.shape[0], :prediction_map.shape[1]][lr, lc]\n",
        "    prediction = np.logical_and(prediction_map, mask)\n",
        "    predictionlist = np.logical_and(prediction_map, mask)[lr, lc]\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    f1_nonstream = f1_score(groundtruthlist, predictionlist, labels=[0], average='micro')\n",
        "    f1_stream = f1_score(groundtruthlist, predictionlist, labels=[1], average='micro')\n",
        "    precision_nonstream = precision_score(groundtruthlist, predictionlist, labels=[0], average='micro')\n",
        "    precision_stream = precision_score(groundtruthlist, predictionlist, labels=[1], average='micro')\n",
        "    recall_nonstream = recall_score(groundtruthlist, predictionlist, labels=[0], average='micro')\n",
        "    recall_stream = recall_score(groundtruthlist, predictionlist, labels=[1], average='micro')\n",
        "    cohen_kappa = cohen_kappa_score(groundtruthlist, predictionlist)\n",
        "\n",
        "    # Print and save evaluation results\n",
        "    print('Model path:', model_path, '  Run at:', str(datetime.now(timezone(timedelta(hours=-6), 'utc'))))\n",
        "    print('F1 score of Nonstream:', str(f1_nonstream))\n",
        "    print('F1 score of Stream:', str(f1_stream))\n",
        "    print('Precision of Nonstream:', str(precision_nonstream))\n",
        "    print('Precision of Stream:', str(precision_stream))\n",
        "    print('Recall of Nonstream:', str(recall_nonstream))\n",
        "    print('Recall of Stream:', str(recall_stream))\n",
        "    print('Cohen Kappa:', str(cohen_kappa))\n"
      ],
      "metadata": {
        "id": "odLJves6k2uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_prediction_result(location, prediction_npy_path, mask_npy, label_npy, save_model_path+classifier+'.pkl')"
      ],
      "metadata": {
        "id": "gNf7V_FdZ-RA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}