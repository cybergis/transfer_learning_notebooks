{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Load all the dependencies\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from numpy import genfromtxt\n",
    "from tensorflow import random\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Layer, UpSampling2D, GlobalAveragePooling2D, Multiply, Dense, Reshape, Permute, multiply, dot, add, Input\n",
    "from keras.layers.core import Dropout, Lambda, SpatialDropout2D, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model, load_model, model_from_yaml, Sequential\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1337) # for reproducibility\n",
    "random.set_seed(1337)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, False)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dice coefficient function as the loss function \n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
    "\n",
    "# Jacard coefficient\n",
    "def jacard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
    "\n",
    "# calculate loss value\n",
    "def jacard_coef_loss(y_true, y_pred):\n",
    "    return -jacard_coef(y_true, y_pred)\n",
    "\n",
    "# calculate loss value\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "def Residual_CNN_block(x, size, dropout=0.0, batch_norm=True):\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        axis = 1\n",
    "    else:\n",
    "        axis = 3\n",
    "    conv = Conv2D(size, (3, 3), padding='same')(x)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Conv2D(size, (3, 3), padding='same')(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Conv2D(size, (3, 3), padding='same')(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    return conv\n",
    "\n",
    "class multiplication(Layer):\n",
    "    def __init__(self,inter_channel = None,**kwargs):\n",
    "        super(multiplication, self).__init__(**kwargs)\n",
    "        self.inter_channel = inter_channel\n",
    "    def build(self,input_shape=None):\n",
    "        self.k = self.add_weight(name='k',shape=(1,),initializer='zeros',dtype='float32',trainable=True)\n",
    "    def get_config(self):\n",
    "        base_config = super(multiplication, self).get_config()\n",
    "        config = {'inter_channel':self.inter_channel}\n",
    "        return dict(list(base_config.items()) + list(config.items()))  \n",
    "    def call(self,inputs):\n",
    "        g,x,x_query,phi_g,x_value = inputs[0],inputs[1],inputs[2],inputs[3],inputs[4]\n",
    "        h,w,c = int(x.shape[1]),int(x.shape[2]),int(x.shape[3])\n",
    "        x_query = K.reshape(x_query, shape=(-1,h*w, self.inter_channel//4))\n",
    "        phi_g = K.reshape(phi_g,shape=(-1,h*w,self.inter_channel//4))\n",
    "        x_value = K.reshape(x_value,shape=(-1,h*w,c))\n",
    "        scale = dot([K.permute_dimensions(phi_g,(0,2,1)), x_query], axes=(1, 2))\n",
    "        soft_scale = Activation('softmax')(scale)\n",
    "        scaled_value = dot([K.permute_dimensions(soft_scale,(0,2,1)),K.permute_dimensions(x_value,(0,2,1))],axes=(1, 2))\n",
    "        scaled_value = K.reshape(scaled_value, shape=(-1,h,w,c))        \n",
    "        customize_multi = self.k * scaled_value\n",
    "        layero = add([customize_multi,x])\n",
    "        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n",
    "        concate = my_concat([layero,g])\n",
    "        return concate \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        ll = list(input_shape)[1]\n",
    "        return (None,ll[1],ll[1],ll[3]*3)\n",
    "    def get_custom_objects():\n",
    "        return {'multiplication': multiplication}\n",
    "\n",
    "def attention_up_and_concatenate(inputs):\n",
    "    g,x = inputs[0],inputs[1]\n",
    "    inter_channel = g.get_shape().as_list()[3]\n",
    "    g = Conv2DTranspose(inter_channel, (2,2), strides=[2, 2],padding='same')(g)\n",
    "    x_query = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(x)\n",
    "    phi_g = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(g)\n",
    "    x_value = Conv2D(inter_channel//2, [1, 1], strides=[1, 1], data_format='channels_last')(x)\n",
    "    inputs = [g,x,x_query,phi_g,x_value]\n",
    "    concate = multiplication(inter_channel)(inputs)\n",
    "    return concate\n",
    "\n",
    "class multiplication2(Layer):\n",
    "    def __init__(self,inter_channel = None,**kwargs):\n",
    "        super(multiplication2, self).__init__(**kwargs)\n",
    "        self.inter_channel = inter_channel\n",
    "    def build(self,input_shape=None):\n",
    "        self.k = self.add_weight(name='k',shape=(1,),initializer='zeros',dtype='float32',trainable=True)\n",
    "    def get_config(self):\n",
    "        base_config = super(multiplication2, self).get_config()\n",
    "        config = {'inter_channel':self.inter_channel}\n",
    "        return dict(list(base_config.items()) + list(config.items()))  \n",
    "    def call(self,inputs):\n",
    "        g,x,rate = inputs[0],inputs[1],inputs[2]\n",
    "        scaled_value = multiply([x, rate])\n",
    "        att_x =  self.k * scaled_value\n",
    "        att_x = add([att_x,x])\n",
    "        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n",
    "        concate = my_concat([att_x, g])\n",
    "        return concate \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        ll = list(input_shape)[1]\n",
    "        return (None,ll[1],ll[1],ll[3]*2)\n",
    "    def get_custom_objects():\n",
    "        return {'multiplication2': multiplication2}\n",
    "\n",
    "def attention_up_and_concatenate2(inputs):\n",
    "    g, x = inputs[0],inputs[1]\n",
    "    inter_channel = g.get_shape().as_list()[3]\n",
    "    g = Conv2DTranspose(inter_channel//2, (3,3), strides=[2, 2],padding='same')(g)\n",
    "    g = Conv2D(inter_channel//2, [1, 1], strides=[1, 1], data_format='channels_last')(g)\n",
    "    theta_x = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(x)\n",
    "    phi_g = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(g)\n",
    "    f = Activation('relu')(add([theta_x, phi_g]))\n",
    "    psi_f = Conv2D(1, [1, 1], strides=[1, 1], data_format='channels_last')(f)\n",
    "    rate = Activation('sigmoid')(psi_f)\n",
    "    concate =  multiplication2()([g,x,rate])\n",
    "    return concate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "Here\n",
      "Here\n",
      "Here\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('June21/model/model_augv_attention2.h5', \n",
    "                             custom_objects={'multiplication': multiplication,'multiplication2': multiplication2, \n",
    "                                             'dice_coef_loss':dice_coef_loss, 'dice_coef':dice_coef,})\n",
    "\n",
    "# remove the last 2 layer using pop() function\n",
    "loaded_model.layers.pop()\n",
    "loaded_model.layers.pop()\n",
    "\n",
    "for (index, layer) in enumerate(loaded_model.layers):\n",
    "    if (index > len(loaded_model.layers)-5):\n",
    "        print(\"Here\")\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "# Create new model from the model using the input and output of the last layer (after poping last 2 layers)\n",
    "# model_without_last = Model(loaded_model.input,  loaded_model.layers[-1].output)\n",
    "model_without_last = Model(loaded_model.input,  loaded_model.output)\n",
    "\n",
    "# model_without_last.trainable = False\n",
    "\n",
    "# See model structure\n",
    "# model_without_last.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of output masks (1 in case you predict only one type of objects)\n",
    "OUTPUT_MASK_CHANNELS = 1\n",
    "\n",
    "# 1 dimensional convolution and generate probabilities from Sigmoid function\n",
    "conv_final = Conv2D(OUTPUT_MASK_CHANNELS, (1, 1), name='conv2d_last')(model_without_last.output)\n",
    "new_out = Activation('sigmoid', name='activation_last')(conv_final)\n",
    "\n",
    "# Created new model with the newly added last two layers \n",
    "transfered_model = Model(inputs=model_without_last.input, outputs=new_out)\n",
    "\n",
    "# New model structure\n",
    "# transfered_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If want to train on the data **without** the NAIP, run the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 224, 224, 8)\n",
      "(600, 224, 224, 8)\n"
     ]
    }
   ],
   "source": [
    "data_path = 'Covington_data/without_NAIP/nodata_as_0/'\n",
    "\n",
    "# read in training and validation data\n",
    "X_train = np.load(data_path+'train_data.npy')\n",
    "Y_train = np.load(data_path+'train_label.npy')\n",
    "X_Validation = np.load(data_path+'vali_data.npy')\n",
    "Y_Validation = np.load(data_path+'vali_label.npy')\n",
    "\n",
    "# The dataset has 9 channels:\n",
    "# 0. Curvature\n",
    "# 1. Slope\n",
    "# 2. Openness\n",
    "# 3. DEM\n",
    "# 4. TPI 21\n",
    "# 5. Reflectance (LiDAR intensity)\n",
    "# 6. Geomorphon\n",
    "# 7. TPI 9\n",
    "# 8. TPI 3\n",
    "# but the model expects 8 channels\n",
    "# So we exclude TPI_9 channel from the data set\n",
    "# X_train_new = X_train[:,:,:,(0,1,2,3,4,5,6,8)]\n",
    "# print(X_train_new.shape)\n",
    "X_train_new = X_train\n",
    "print(X_train_new.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[:,:,:,(0,1,2,3,4,5,6,8)]\n",
    "# print(X_Validation_new.shape)\n",
    "\n",
    "X_Validation_new = X_Validation\n",
    "print(X_Validation_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If want to train on the data **with** the NAIP, run the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Covington_data/include_NAIP/nodata_as_0/500_samples/'\n",
    "\n",
    "# read in training and validation sample patches\n",
    "X_train_new = np.load(data_path+'train_data.npy')\n",
    "X_Validation_new = np.load(data_path+'vali_data.npy')\n",
    "print(X_train_new.shape)\n",
    "print(X_Validation_new.shape)\n",
    "\n",
    "#Read training and validation labels\n",
    "Y_Validation = np.load(data_path+'vali_label.npy')\n",
    "Y_train = np.load(data_path+'train_label.npy')\n",
    "\n",
    "#Cast both labales to float32\n",
    "Y_Validation = Y_Validation.astype(np.float32)\n",
    "Y_train = Y_train.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 224\n",
    "IMG_WIDTH = patch_size\n",
    "IMG_HEIGHT = patch_size\n",
    "# Number of feature channels \n",
    "INPUT_CHANNELS = 8\n",
    "# Number of output masks (1 in case you predict only one type of objects)\n",
    "OUTPUT_MASK_CHANNELS = 1\n",
    "maxepoch = 100\n",
    "# hyperparameters\n",
    "# learning_rate = 0.0000359\n",
    "learning_rate = 0.0001\n",
    "patience = 20\n",
    "aug = 'v'\n",
    "transfered_model.compile(optimizer=Adam(lr=learning_rate),loss = dice_coef_loss,metrics=[dice_coef,'accuracy'])\n",
    "callbacks = [\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=patience, min_lr=1e-9, verbose=1, mode='min'),\n",
    "        EarlyStopping(monitor='val_loss', patience=patience+10, verbose=0),\n",
    "        ModelCheckpoint('model'+aug+'_attention2.h5', monitor='val_loss', save_best_only=True, verbose=0),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 21s 35ms/step - loss: -0.0696 - dice_coef: 0.0696 - accuracy: 0.0366 - val_loss: -0.0447 - val_dice_coef: 0.0447 - val_accuracy: 0.0226\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 19s 31ms/step - loss: -0.0703 - dice_coef: 0.0703 - accuracy: 0.0366 - val_loss: -0.0450 - val_dice_coef: 0.0450 - val_accuracy: 0.0226\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0709 - dice_coef: 0.0709 - accuracy: 0.0366 - val_loss: -0.0453 - val_dice_coef: 0.0453 - val_accuracy: 0.0226\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0711 - dice_coef: 0.0711 - accuracy: 0.0366 - val_loss: -0.0455 - val_dice_coef: 0.0455 - val_accuracy: 0.0226\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0720 - dice_coef: 0.0720 - accuracy: 0.0366 - val_loss: -0.0456 - val_dice_coef: 0.0456 - val_accuracy: 0.0226\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0722 - dice_coef: 0.0722 - accuracy: 0.0366 - val_loss: -0.0460 - val_dice_coef: 0.0460 - val_accuracy: 0.0226\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0728 - dice_coef: 0.0728 - accuracy: 0.0366 - val_loss: -0.0464 - val_dice_coef: 0.0464 - val_accuracy: 0.0226\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0735 - dice_coef: 0.0735 - accuracy: 0.0366 - val_loss: -0.0468 - val_dice_coef: 0.0468 - val_accuracy: 0.0226\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0739 - dice_coef: 0.0739 - accuracy: 0.1817 - val_loss: -0.0470 - val_dice_coef: 0.0470 - val_accuracy: 0.4181\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0746 - dice_coef: 0.0746 - accuracy: 0.8689 - val_loss: -0.0472 - val_dice_coef: 0.0472 - val_accuracy: 0.6792\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0751 - dice_coef: 0.0751 - accuracy: 0.8935 - val_loss: -0.0477 - val_dice_coef: 0.0477 - val_accuracy: 0.8988\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0756 - dice_coef: 0.0756 - accuracy: 0.8934 - val_loss: -0.0475 - val_dice_coef: 0.0475 - val_accuracy: 0.6140\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0762 - dice_coef: 0.0762 - accuracy: 0.8977 - val_loss: -0.0481 - val_dice_coef: 0.0481 - val_accuracy: 0.7808\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0770 - dice_coef: 0.0770 - accuracy: 0.9047 - val_loss: -0.0477 - val_dice_coef: 0.0477 - val_accuracy: 0.5844\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0775 - dice_coef: 0.0775 - accuracy: 0.9048 - val_loss: -0.0489 - val_dice_coef: 0.0489 - val_accuracy: 0.8241\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0781 - dice_coef: 0.0781 - accuracy: 0.9080 - val_loss: -0.0490 - val_dice_coef: 0.0490 - val_accuracy: 0.7765\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0783 - dice_coef: 0.0783 - accuracy: 0.9085 - val_loss: -0.0481 - val_dice_coef: 0.0481 - val_accuracy: 0.5415\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0789 - dice_coef: 0.0789 - accuracy: 0.9100 - val_loss: -0.0497 - val_dice_coef: 0.0497 - val_accuracy: 0.7993\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0800 - dice_coef: 0.0800 - accuracy: 0.9111 - val_loss: -0.0496 - val_dice_coef: 0.0496 - val_accuracy: 0.7202\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0807 - dice_coef: 0.0807 - accuracy: 0.9113 - val_loss: -0.0505 - val_dice_coef: 0.0505 - val_accuracy: 0.8216\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0813 - dice_coef: 0.0813 - accuracy: 0.9109 - val_loss: -0.0502 - val_dice_coef: 0.0502 - val_accuracy: 0.7175\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0819 - dice_coef: 0.0819 - accuracy: 0.9121 - val_loss: -0.0504 - val_dice_coef: 0.0504 - val_accuracy: 0.7058\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0825 - dice_coef: 0.0825 - accuracy: 0.9129 - val_loss: -0.0512 - val_dice_coef: 0.0512 - val_accuracy: 0.7673\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0834 - dice_coef: 0.0834 - accuracy: 0.9114 - val_loss: -0.0520 - val_dice_coef: 0.0520 - val_accuracy: 0.8150\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0844 - dice_coef: 0.0844 - accuracy: 0.9131 - val_loss: -0.0523 - val_dice_coef: 0.0523 - val_accuracy: 0.8121\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0846 - dice_coef: 0.0846 - accuracy: 0.9134 - val_loss: -0.0530 - val_dice_coef: 0.0530 - val_accuracy: 0.8401\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0854 - dice_coef: 0.0854 - accuracy: 0.9139 - val_loss: -0.0517 - val_dice_coef: 0.0517 - val_accuracy: 0.6856\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0861 - dice_coef: 0.0861 - accuracy: 0.9138 - val_loss: -0.0523 - val_dice_coef: 0.0523 - val_accuracy: 0.7150\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0873 - dice_coef: 0.0873 - accuracy: 0.9167 - val_loss: -0.0530 - val_dice_coef: 0.0530 - val_accuracy: 0.7458\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0881 - dice_coef: 0.0881 - accuracy: 0.9166 - val_loss: -0.0557 - val_dice_coef: 0.0557 - val_accuracy: 0.9242\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0894 - dice_coef: 0.0894 - accuracy: 0.9156 - val_loss: -0.0552 - val_dice_coef: 0.0552 - val_accuracy: 0.8481\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0905 - dice_coef: 0.0905 - accuracy: 0.9174 - val_loss: -0.0561 - val_dice_coef: 0.0561 - val_accuracy: 0.8770\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0912 - dice_coef: 0.0912 - accuracy: 0.9175 - val_loss: -0.0545 - val_dice_coef: 0.0545 - val_accuracy: 0.7492\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0922 - dice_coef: 0.0922 - accuracy: 0.9200 - val_loss: -0.0537 - val_dice_coef: 0.0537 - val_accuracy: 0.6812\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0934 - dice_coef: 0.0934 - accuracy: 0.9191 - val_loss: -0.0559 - val_dice_coef: 0.0559 - val_accuracy: 0.7846\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0945 - dice_coef: 0.0945 - accuracy: 0.9201 - val_loss: -0.0586 - val_dice_coef: 0.0586 - val_accuracy: 0.9056\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0954 - dice_coef: 0.0954 - accuracy: 0.9196 - val_loss: -0.0537 - val_dice_coef: 0.0537 - val_accuracy: 0.6242\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0967 - dice_coef: 0.0967 - accuracy: 0.9217 - val_loss: -0.0598 - val_dice_coef: 0.0598 - val_accuracy: 0.9068\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0981 - dice_coef: 0.0981 - accuracy: 0.9214 - val_loss: -0.0566 - val_dice_coef: 0.0566 - val_accuracy: 0.7404\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0984 - dice_coef: 0.0984 - accuracy: 0.9200 - val_loss: -0.0603 - val_dice_coef: 0.0603 - val_accuracy: 0.8796\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.0997 - dice_coef: 0.0997 - accuracy: 0.9212 - val_loss: -0.0611 - val_dice_coef: 0.0611 - val_accuracy: 0.8886\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1013 - dice_coef: 0.1013 - accuracy: 0.9218 - val_loss: -0.0610 - val_dice_coef: 0.0610 - val_accuracy: 0.8628\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1020 - dice_coef: 0.1020 - accuracy: 0.9219 - val_loss: -0.0618 - val_dice_coef: 0.0618 - val_accuracy: 0.8763\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1035 - dice_coef: 0.1035 - accuracy: 0.9222 - val_loss: -0.0623 - val_dice_coef: 0.0623 - val_accuracy: 0.8721\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1055 - dice_coef: 0.1055 - accuracy: 0.9232 - val_loss: -0.0629 - val_dice_coef: 0.0629 - val_accuracy: 0.8713\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1064 - dice_coef: 0.1064 - accuracy: 0.9236 - val_loss: -0.0645 - val_dice_coef: 0.0645 - val_accuracy: 0.8969\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1075 - dice_coef: 0.1075 - accuracy: 0.9259 - val_loss: -0.0647 - val_dice_coef: 0.0647 - val_accuracy: 0.8937\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1088 - dice_coef: 0.1088 - accuracy: 0.9239 - val_loss: -0.0646 - val_dice_coef: 0.0646 - val_accuracy: 0.8701\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1094 - dice_coef: 0.1094 - accuracy: 0.9256 - val_loss: -0.0634 - val_dice_coef: 0.0634 - val_accuracy: 0.8428\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1108 - dice_coef: 0.1108 - accuracy: 0.9253 - val_loss: -0.0671 - val_dice_coef: 0.0671 - val_accuracy: 0.9062\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1122 - dice_coef: 0.1122 - accuracy: 0.9259 - val_loss: -0.0640 - val_dice_coef: 0.0640 - val_accuracy: 0.8278\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1138 - dice_coef: 0.1138 - accuracy: 0.9250 - val_loss: -0.0692 - val_dice_coef: 0.0692 - val_accuracy: 0.9135\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1164 - dice_coef: 0.1164 - accuracy: 0.9273 - val_loss: -0.0682 - val_dice_coef: 0.0682 - val_accuracy: 0.8880\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1165 - dice_coef: 0.1165 - accuracy: 0.9252 - val_loss: -0.0682 - val_dice_coef: 0.0682 - val_accuracy: 0.8708\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1174 - dice_coef: 0.1174 - accuracy: 0.9273 - val_loss: -0.0676 - val_dice_coef: 0.0676 - val_accuracy: 0.8529\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1185 - dice_coef: 0.1185 - accuracy: 0.9271 - val_loss: -0.0690 - val_dice_coef: 0.0690 - val_accuracy: 0.8660\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1208 - dice_coef: 0.1208 - accuracy: 0.9279 - val_loss: -0.0696 - val_dice_coef: 0.0696 - val_accuracy: 0.8661\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1224 - dice_coef: 0.1224 - accuracy: 0.9290 - val_loss: -0.0719 - val_dice_coef: 0.0719 - val_accuracy: 0.8922\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1250 - dice_coef: 0.1250 - accuracy: 0.9311 - val_loss: -0.0736 - val_dice_coef: 0.0736 - val_accuracy: 0.9049\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1250 - dice_coef: 0.1250 - accuracy: 0.9298 - val_loss: -0.0730 - val_dice_coef: 0.0730 - val_accuracy: 0.8895\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1275 - dice_coef: 0.1275 - accuracy: 0.9307 - val_loss: -0.0728 - val_dice_coef: 0.0728 - val_accuracy: 0.8783\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1287 - dice_coef: 0.1287 - accuracy: 0.9310 - val_loss: -0.0737 - val_dice_coef: 0.0737 - val_accuracy: 0.8900\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 20s 33ms/step - loss: -0.1310 - dice_coef: 0.1310 - accuracy: 0.9320 - val_loss: -0.0758 - val_dice_coef: 0.0758 - val_accuracy: 0.8960\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 20s 33ms/step - loss: -0.1322 - dice_coef: 0.1322 - accuracy: 0.9329 - val_loss: -0.0767 - val_dice_coef: 0.0767 - val_accuracy: 0.9045\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1326 - dice_coef: 0.1326 - accuracy: 0.9325 - val_loss: -0.0748 - val_dice_coef: 0.0748 - val_accuracy: 0.8755\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1343 - dice_coef: 0.1343 - accuracy: 0.9309 - val_loss: -0.0777 - val_dice_coef: 0.0777 - val_accuracy: 0.8947\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 20s 33ms/step - loss: -0.1372 - dice_coef: 0.1372 - accuracy: 0.9335 - val_loss: -0.0827 - val_dice_coef: 0.0827 - val_accuracy: 0.9322\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1374 - dice_coef: 0.1374 - accuracy: 0.9338 - val_loss: -0.0818 - val_dice_coef: 0.0818 - val_accuracy: 0.9195\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1396 - dice_coef: 0.1396 - accuracy: 0.9323 - val_loss: -0.0837 - val_dice_coef: 0.0837 - val_accuracy: 0.9263\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1418 - dice_coef: 0.1418 - accuracy: 0.9346 - val_loss: -0.0844 - val_dice_coef: 0.0844 - val_accuracy: 0.9262\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1439 - dice_coef: 0.1439 - accuracy: 0.9360 - val_loss: -0.0871 - val_dice_coef: 0.0871 - val_accuracy: 0.9405\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1441 - dice_coef: 0.1441 - accuracy: 0.9339 - val_loss: -0.0861 - val_dice_coef: 0.0861 - val_accuracy: 0.9269\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1460 - dice_coef: 0.1460 - accuracy: 0.9357 - val_loss: -0.0878 - val_dice_coef: 0.0878 - val_accuracy: 0.9296\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1485 - dice_coef: 0.1485 - accuracy: 0.9360 - val_loss: -0.0875 - val_dice_coef: 0.0875 - val_accuracy: 0.9282\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1517 - dice_coef: 0.1517 - accuracy: 0.9365 - val_loss: -0.0888 - val_dice_coef: 0.0888 - val_accuracy: 0.9278\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1522 - dice_coef: 0.1522 - accuracy: 0.9359 - val_loss: -0.0908 - val_dice_coef: 0.0908 - val_accuracy: 0.9328\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1541 - dice_coef: 0.1541 - accuracy: 0.9372 - val_loss: -0.0926 - val_dice_coef: 0.0926 - val_accuracy: 0.9389\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1549 - dice_coef: 0.1549 - accuracy: 0.9363 - val_loss: -0.0951 - val_dice_coef: 0.0951 - val_accuracy: 0.9455\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1593 - dice_coef: 0.1593 - accuracy: 0.9394 - val_loss: -0.0948 - val_dice_coef: 0.0948 - val_accuracy: 0.9408\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1596 - dice_coef: 0.1596 - accuracy: 0.9393 - val_loss: -0.0959 - val_dice_coef: 0.0959 - val_accuracy: 0.9420\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1611 - dice_coef: 0.1611 - accuracy: 0.9378 - val_loss: -0.0978 - val_dice_coef: 0.0978 - val_accuracy: 0.9425\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1636 - dice_coef: 0.1636 - accuracy: 0.9399 - val_loss: -0.0988 - val_dice_coef: 0.0988 - val_accuracy: 0.9428\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1661 - dice_coef: 0.1661 - accuracy: 0.9401 - val_loss: -0.0946 - val_dice_coef: 0.0946 - val_accuracy: 0.9242\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1679 - dice_coef: 0.1679 - accuracy: 0.9396 - val_loss: -0.1020 - val_dice_coef: 0.1020 - val_accuracy: 0.9475\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1693 - dice_coef: 0.1693 - accuracy: 0.9406 - val_loss: -0.1007 - val_dice_coef: 0.1007 - val_accuracy: 0.9430\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1713 - dice_coef: 0.1713 - accuracy: 0.9411 - val_loss: -0.1015 - val_dice_coef: 0.1015 - val_accuracy: 0.9401\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1734 - dice_coef: 0.1734 - accuracy: 0.9410 - val_loss: -0.1052 - val_dice_coef: 0.1052 - val_accuracy: 0.9471\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1753 - dice_coef: 0.1753 - accuracy: 0.9421 - val_loss: -0.1064 - val_dice_coef: 0.1064 - val_accuracy: 0.9489\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1770 - dice_coef: 0.1770 - accuracy: 0.9431 - val_loss: -0.1086 - val_dice_coef: 0.1086 - val_accuracy: 0.9512\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 20s 33ms/step - loss: -0.1810 - dice_coef: 0.1810 - accuracy: 0.9444 - val_loss: -0.1097 - val_dice_coef: 0.1097 - val_accuracy: 0.9532\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1826 - dice_coef: 0.1826 - accuracy: 0.9439 - val_loss: -0.1107 - val_dice_coef: 0.1107 - val_accuracy: 0.9501\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1822 - dice_coef: 0.1822 - accuracy: 0.9441 - val_loss: -0.1117 - val_dice_coef: 0.1117 - val_accuracy: 0.9497\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1871 - dice_coef: 0.1871 - accuracy: 0.9456 - val_loss: -0.1147 - val_dice_coef: 0.1147 - val_accuracy: 0.9569\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1887 - dice_coef: 0.1887 - accuracy: 0.9461 - val_loss: -0.1147 - val_dice_coef: 0.1147 - val_accuracy: 0.9526\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1916 - dice_coef: 0.1916 - accuracy: 0.9467 - val_loss: -0.1163 - val_dice_coef: 0.1163 - val_accuracy: 0.9554\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1908 - dice_coef: 0.1908 - accuracy: 0.9458 - val_loss: -0.1167 - val_dice_coef: 0.1167 - val_accuracy: 0.9500\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1952 - dice_coef: 0.1952 - accuracy: 0.9465 - val_loss: -0.1196 - val_dice_coef: 0.1196 - val_accuracy: 0.9566\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.1961 - dice_coef: 0.1961 - accuracy: 0.9470 - val_loss: -0.1198 - val_dice_coef: 0.1198 - val_accuracy: 0.9535\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.2005 - dice_coef: 0.2005 - accuracy: 0.9489 - val_loss: -0.1214 - val_dice_coef: 0.1214 - val_accuracy: 0.9533\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 19s 32ms/step - loss: -0.2014 - dice_coef: 0.2014 - accuracy: 0.9491 - val_loss: -0.1235 - val_dice_coef: 0.1235 - val_accuracy: 0.9573\n"
     ]
    }
   ],
   "source": [
    "ltranfer_learning_results = transfered_model.fit(X_train_new, Y_train, validation_data=(X_Validation_new,Y_Validation), batch_size=2, epochs=maxepoch, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "root_path = './training_results/Without_NAIP/'\n",
    "# save the trained model\n",
    "model_yaml = transfered_model.to_yaml()\n",
    "with open(root_path+\"model_transfere-_learning_No_NAIP_\"+timestr+\".yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "# save the weights\n",
    "transfered_model.save(root_path+\"model_transfere-_learning_No_NAIP_\"+timestr+\".h5\")\n",
    "# save the intermdediate results and training statistics\n",
    "with open(root_path+\"history_transfere-_learning_No_NAIP_\"+timestr+\".pickle\", 'wb') as file_pi:\n",
    "    pickle.dump(ltranfer_learning_results.history, file_pi, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will wait for the whole area data to do prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './training_results/Without_NAIP/'\n",
    "loaded_model = load_model(root_path+\"model_transfere-_learning_No_NAIP_20201115-234208.h5\", \n",
    "                             custom_objects={'multiplication': multiplication,'multiplication2': multiplication2, \n",
    "                                             'dice_coef_loss':dice_coef_loss, 'dice_coef':dice_coef,})\n",
    "\n",
    "for (index, layer) in enumerate(loaded_model.layers):\n",
    "    layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 224\n",
    "IMG_WIDTH = patch_size\n",
    "IMG_HEIGHT = patch_size\n",
    "# Number of feature channels \n",
    "INPUT_CHANNELS = 8\n",
    "# Number of output masks (1 in case you predict only one type of objects)\n",
    "OUTPUT_MASK_CHANNELS = 1\n",
    "maxepoch = 100\n",
    "# hyperparameters\n",
    "# 100 times smaller learning rate\n",
    "learning_rate = 0.0000359\n",
    "patience = 20\n",
    "aug = 'v'\n",
    "loaded_model.compile(optimizer=Adam(lr=learning_rate),loss = dice_coef_loss,metrics=[dice_coef,'accuracy'])\n",
    "callbacks = [\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=patience, min_lr=1e-9, verbose=1, mode='min'),\n",
    "        EarlyStopping(monitor='val_loss', patience=patience+10, verbose=0),\n",
    "        ModelCheckpoint('model'+aug+'_attention2.h5', monitor='val_loss', save_best_only=True, verbose=0),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 65s 108ms/step - loss: -0.2325 - dice_coef: 0.2325 - accuracy: 0.9608 - val_loss: -0.1386 - val_dice_coef: 0.1386 - val_accuracy: 0.9822\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.2453 - dice_coef: 0.2453 - accuracy: 0.9673 - val_loss: -0.1501 - val_dice_coef: 0.1501 - val_accuracy: 0.9744\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.2519 - dice_coef: 0.2519 - accuracy: 0.9707 - val_loss: -0.1473 - val_dice_coef: 0.1473 - val_accuracy: 0.9742\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 51s 85ms/step - loss: -0.2546 - dice_coef: 0.2546 - accuracy: 0.9720 - val_loss: -0.1481 - val_dice_coef: 0.1481 - val_accuracy: 0.9789\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.2619 - dice_coef: 0.2619 - accuracy: 0.9749 - val_loss: -0.1374 - val_dice_coef: 0.1374 - val_accuracy: 0.9765\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.2626 - dice_coef: 0.2626 - accuracy: 0.9755 - val_loss: -0.1507 - val_dice_coef: 0.1507 - val_accuracy: 0.9733\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 51s 85ms/step - loss: -0.2679 - dice_coef: 0.2679 - accuracy: 0.9781 - val_loss: -0.1474 - val_dice_coef: 0.1474 - val_accuracy: 0.9730\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.2749 - dice_coef: 0.2749 - accuracy: 0.9806 - val_loss: -0.1509 - val_dice_coef: 0.1509 - val_accuracy: 0.9732\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 51s 85ms/step - loss: -0.2767 - dice_coef: 0.2767 - accuracy: 0.9807 - val_loss: -0.1425 - val_dice_coef: 0.1425 - val_accuracy: 0.9772\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 51s 85ms/step - loss: -0.2831 - dice_coef: 0.2831 - accuracy: 0.9838 - val_loss: -0.1525 - val_dice_coef: 0.1525 - val_accuracy: 0.9728\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 52s 87ms/step - loss: -0.2833 - dice_coef: 0.2833 - accuracy: 0.9826 - val_loss: -0.1544 - val_dice_coef: 0.1544 - val_accuracy: 0.9689\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.2858 - dice_coef: 0.2858 - accuracy: 0.9834 - val_loss: -0.1464 - val_dice_coef: 0.1464 - val_accuracy: 0.9787\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.2882 - dice_coef: 0.2882 - accuracy: 0.9845 - val_loss: -0.1580 - val_dice_coef: 0.1580 - val_accuracy: 0.9744\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.2974 - dice_coef: 0.2974 - accuracy: 0.9872 - val_loss: -0.1527 - val_dice_coef: 0.1527 - val_accuracy: 0.9770\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3005 - dice_coef: 0.3005 - accuracy: 0.9879 - val_loss: -0.1484 - val_dice_coef: 0.1484 - val_accuracy: 0.9750\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3018 - dice_coef: 0.3018 - accuracy: 0.9884 - val_loss: -0.1606 - val_dice_coef: 0.1606 - val_accuracy: 0.9811\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.2983 - dice_coef: 0.2983 - accuracy: 0.9878 - val_loss: -0.1523 - val_dice_coef: 0.1523 - val_accuracy: 0.9770\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3025 - dice_coef: 0.3025 - accuracy: 0.9889 - val_loss: -0.1579 - val_dice_coef: 0.1579 - val_accuracy: 0.9763\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.3062 - dice_coef: 0.3062 - accuracy: 0.9888 - val_loss: -0.1470 - val_dice_coef: 0.1470 - val_accuracy: 0.9794\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3096 - dice_coef: 0.3096 - accuracy: 0.9889 - val_loss: -0.1493 - val_dice_coef: 0.1493 - val_accuracy: 0.9795\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3106 - dice_coef: 0.3106 - accuracy: 0.9899 - val_loss: -0.1474 - val_dice_coef: 0.1474 - val_accuracy: 0.9797\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3098 - dice_coef: 0.3098 - accuracy: 0.9896 - val_loss: -0.1425 - val_dice_coef: 0.1425 - val_accuracy: 0.9817\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3103 - dice_coef: 0.3103 - accuracy: 0.9887 - val_loss: -0.1617 - val_dice_coef: 0.1617 - val_accuracy: 0.9766\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.3138 - dice_coef: 0.3138 - accuracy: 0.9896 - val_loss: -0.1546 - val_dice_coef: 0.1546 - val_accuracy: 0.9834\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 51s 85ms/step - loss: -0.3196 - dice_coef: 0.3196 - accuracy: 0.9896 - val_loss: -0.1561 - val_dice_coef: 0.1561 - val_accuracy: 0.9793\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3176 - dice_coef: 0.3176 - accuracy: 0.9904 - val_loss: -0.1503 - val_dice_coef: 0.1503 - val_accuracy: 0.9814\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 51s 85ms/step - loss: -0.3162 - dice_coef: 0.3162 - accuracy: 0.9914 - val_loss: -0.1628 - val_dice_coef: 0.1628 - val_accuracy: 0.9784\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3190 - dice_coef: 0.3190 - accuracy: 0.9907 - val_loss: -0.1499 - val_dice_coef: 0.1499 - val_accuracy: 0.9793\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3240 - dice_coef: 0.3240 - accuracy: 0.9912 - val_loss: -0.1516 - val_dice_coef: 0.1516 - val_accuracy: 0.9823\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.3268 - dice_coef: 0.3268 - accuracy: 0.9920 - val_loss: -0.1537 - val_dice_coef: 0.1537 - val_accuracy: 0.9820\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 51s 85ms/step - loss: -0.3302 - dice_coef: 0.3302 - accuracy: 0.9914 - val_loss: -0.1507 - val_dice_coef: 0.1507 - val_accuracy: 0.9824\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3340 - dice_coef: 0.3340 - accuracy: 0.9924 - val_loss: -0.1500 - val_dice_coef: 0.1500 - val_accuracy: 0.9832\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3311 - dice_coef: 0.3311 - accuracy: 0.9916 - val_loss: -0.1540 - val_dice_coef: 0.1540 - val_accuracy: 0.9837\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3314 - dice_coef: 0.3314 - accuracy: 0.9915 - val_loss: -0.1673 - val_dice_coef: 0.1673 - val_accuracy: 0.9772\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3308 - dice_coef: 0.3308 - accuracy: 0.9917 - val_loss: -0.1582 - val_dice_coef: 0.1582 - val_accuracy: 0.9838\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3374 - dice_coef: 0.3374 - accuracy: 0.9931 - val_loss: -0.1656 - val_dice_coef: 0.1656 - val_accuracy: 0.9854\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3385 - dice_coef: 0.3385 - accuracy: 0.9935 - val_loss: -0.1605 - val_dice_coef: 0.1605 - val_accuracy: 0.9796\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.3395 - dice_coef: 0.3395 - accuracy: 0.9934 - val_loss: -0.1727 - val_dice_coef: 0.1727 - val_accuracy: 0.9781\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3447 - dice_coef: 0.3447 - accuracy: 0.9938 - val_loss: -0.1601 - val_dice_coef: 0.1601 - val_accuracy: 0.9808\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3418 - dice_coef: 0.3418 - accuracy: 0.9936 - val_loss: -0.1684 - val_dice_coef: 0.1684 - val_accuracy: 0.9848\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3436 - dice_coef: 0.3436 - accuracy: 0.9928 - val_loss: -0.1685 - val_dice_coef: 0.1685 - val_accuracy: 0.9758\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3462 - dice_coef: 0.3462 - accuracy: 0.9937 - val_loss: -0.1684 - val_dice_coef: 0.1684 - val_accuracy: 0.9728\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3435 - dice_coef: 0.3435 - accuracy: 0.9927 - val_loss: -0.1684 - val_dice_coef: 0.1684 - val_accuracy: 0.9798\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3485 - dice_coef: 0.3485 - accuracy: 0.9937 - val_loss: -0.1687 - val_dice_coef: 0.1687 - val_accuracy: 0.9854\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3573 - dice_coef: 0.3573 - accuracy: 0.9944 - val_loss: -0.1752 - val_dice_coef: 0.1752 - val_accuracy: 0.9721\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3551 - dice_coef: 0.3551 - accuracy: 0.9934 - val_loss: -0.1474 - val_dice_coef: 0.1474 - val_accuracy: 0.9846\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.3524 - dice_coef: 0.3524 - accuracy: 0.9942 - val_loss: -0.1629 - val_dice_coef: 0.1629 - val_accuracy: 0.9827\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3561 - dice_coef: 0.3561 - accuracy: 0.9948 - val_loss: -0.1674 - val_dice_coef: 0.1674 - val_accuracy: 0.9842\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3564 - dice_coef: 0.3564 - accuracy: 0.9952 - val_loss: -0.1635 - val_dice_coef: 0.1635 - val_accuracy: 0.9821\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.3528 - dice_coef: 0.3528 - accuracy: 0.9936 - val_loss: -0.1699 - val_dice_coef: 0.1699 - val_accuracy: 0.9759\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3604 - dice_coef: 0.3604 - accuracy: 0.9950 - val_loss: -0.1531 - val_dice_coef: 0.1531 - val_accuracy: 0.9845\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3622 - dice_coef: 0.3622 - accuracy: 0.9952 - val_loss: -0.1672 - val_dice_coef: 0.1672 - val_accuracy: 0.9852\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3730 - dice_coef: 0.3730 - accuracy: 0.9962 - val_loss: -0.1618 - val_dice_coef: 0.1618 - val_accuracy: 0.9848\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 51s 85ms/step - loss: -0.3632 - dice_coef: 0.3632 - accuracy: 0.9950 - val_loss: -0.1635 - val_dice_coef: 0.1635 - val_accuracy: 0.9823\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3654 - dice_coef: 0.3654 - accuracy: 0.9960 - val_loss: -0.1636 - val_dice_coef: 0.1636 - val_accuracy: 0.9828\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 51s 85ms/step - loss: -0.3662 - dice_coef: 0.3662 - accuracy: 0.9959 - val_loss: -0.1636 - val_dice_coef: 0.1636 - val_accuracy: 0.9809\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.3701 - dice_coef: 0.3701 - accuracy: 0.9953 - val_loss: -0.1419 - val_dice_coef: 0.1419 - val_accuracy: 0.9792\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3695 - dice_coef: 0.3695 - accuracy: 0.9951 - val_loss: -0.1820 - val_dice_coef: 0.1820 - val_accuracy: 0.9826\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 51s 85ms/step - loss: -0.3794 - dice_coef: 0.3794 - accuracy: 0.9956 - val_loss: -0.1762 - val_dice_coef: 0.1762 - val_accuracy: 0.9825\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3765 - dice_coef: 0.3765 - accuracy: 0.9962 - val_loss: -0.1741 - val_dice_coef: 0.1741 - val_accuracy: 0.9844\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 51s 85ms/step - loss: -0.3792 - dice_coef: 0.3792 - accuracy: 0.9963 - val_loss: -0.1819 - val_dice_coef: 0.1819 - val_accuracy: 0.9832\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3802 - dice_coef: 0.3802 - accuracy: 0.9962 - val_loss: -0.1786 - val_dice_coef: 0.1786 - val_accuracy: 0.9848\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3832 - dice_coef: 0.3832 - accuracy: 0.9967 - val_loss: -0.1752 - val_dice_coef: 0.1752 - val_accuracy: 0.9835\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3849 - dice_coef: 0.3849 - accuracy: 0.9966 - val_loss: -0.1767 - val_dice_coef: 0.1767 - val_accuracy: 0.9849\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3783 - dice_coef: 0.3783 - accuracy: 0.9949 - val_loss: -0.1824 - val_dice_coef: 0.1824 - val_accuracy: 0.9836\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 51s 85ms/step - loss: -0.3823 - dice_coef: 0.3823 - accuracy: 0.9959 - val_loss: -0.1788 - val_dice_coef: 0.1788 - val_accuracy: 0.9837\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 52s 87ms/step - loss: -0.3864 - dice_coef: 0.3864 - accuracy: 0.9964 - val_loss: -0.1802 - val_dice_coef: 0.1802 - val_accuracy: 0.9816\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 52s 87ms/step - loss: -0.3851 - dice_coef: 0.3851 - accuracy: 0.9961 - val_loss: -0.1826 - val_dice_coef: 0.1826 - val_accuracy: 0.9857\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 52s 87ms/step - loss: -0.3856 - dice_coef: 0.3856 - accuracy: 0.9966 - val_loss: -0.1847 - val_dice_coef: 0.1847 - val_accuracy: 0.9847\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.3918 - dice_coef: 0.3918 - accuracy: 0.9967 - val_loss: -0.1825 - val_dice_coef: 0.1825 - val_accuracy: 0.9843\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.3951 - dice_coef: 0.3951 - accuracy: 0.9971 - val_loss: -0.1758 - val_dice_coef: 0.1758 - val_accuracy: 0.9847\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.3900 - dice_coef: 0.3900 - accuracy: 0.9968 - val_loss: -0.1895 - val_dice_coef: 0.1895 - val_accuracy: 0.9857\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.3915 - dice_coef: 0.3915 - accuracy: 0.9967 - val_loss: -0.1863 - val_dice_coef: 0.1863 - val_accuracy: 0.9837\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3964 - dice_coef: 0.3964 - accuracy: 0.9967 - val_loss: -0.1877 - val_dice_coef: 0.1877 - val_accuracy: 0.9850\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.3976 - dice_coef: 0.3976 - accuracy: 0.9968 - val_loss: -0.1738 - val_dice_coef: 0.1738 - val_accuracy: 0.9851\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.3997 - dice_coef: 0.3997 - accuracy: 0.9973 - val_loss: -0.1854 - val_dice_coef: 0.1854 - val_accuracy: 0.9848\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.4030 - dice_coef: 0.4030 - accuracy: 0.9974 - val_loss: -0.1766 - val_dice_coef: 0.1766 - val_accuracy: 0.9846\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.3985 - dice_coef: 0.3985 - accuracy: 0.9974 - val_loss: -0.2050 - val_dice_coef: 0.2050 - val_accuracy: 0.9847\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.4059 - dice_coef: 0.4059 - accuracy: 0.9966 - val_loss: -0.1864 - val_dice_coef: 0.1864 - val_accuracy: 0.9842\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.4033 - dice_coef: 0.4033 - accuracy: 0.9974 - val_loss: -0.1872 - val_dice_coef: 0.1872 - val_accuracy: 0.9854\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.4058 - dice_coef: 0.4058 - accuracy: 0.9976 - val_loss: -0.1717 - val_dice_coef: 0.1717 - val_accuracy: 0.9838\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.4035 - dice_coef: 0.4035 - accuracy: 0.9961 - val_loss: -0.1933 - val_dice_coef: 0.1933 - val_accuracy: 0.9843\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.4120 - dice_coef: 0.4120 - accuracy: 0.9972 - val_loss: -0.1858 - val_dice_coef: 0.1858 - val_accuracy: 0.9853\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.4169 - dice_coef: 0.4169 - accuracy: 0.9976 - val_loss: -0.1922 - val_dice_coef: 0.1922 - val_accuracy: 0.9858\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.4107 - dice_coef: 0.4107 - accuracy: 0.9977 - val_loss: -0.1879 - val_dice_coef: 0.1879 - val_accuracy: 0.9850\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.4124 - dice_coef: 0.4124 - accuracy: 0.9974 - val_loss: -0.1752 - val_dice_coef: 0.1752 - val_accuracy: 0.9840\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.4135 - dice_coef: 0.4135 - accuracy: 0.9972 - val_loss: -0.1937 - val_dice_coef: 0.1937 - val_accuracy: 0.9852\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.4167 - dice_coef: 0.4167 - accuracy: 0.9976 - val_loss: -0.1899 - val_dice_coef: 0.1899 - val_accuracy: 0.9855\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.4190 - dice_coef: 0.4190 - accuracy: 0.9977 - val_loss: -0.2011 - val_dice_coef: 0.2011 - val_accuracy: 0.9826\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.4244 - dice_coef: 0.4244 - accuracy: 0.9979 - val_loss: -0.1880 - val_dice_coef: 0.1880 - val_accuracy: 0.9860\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.4230 - dice_coef: 0.4230 - accuracy: 0.9980 - val_loss: -0.1972 - val_dice_coef: 0.1972 - val_accuracy: 0.9860\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.4215 - dice_coef: 0.4215 - accuracy: 0.9980 - val_loss: -0.1959 - val_dice_coef: 0.1959 - val_accuracy: 0.9860\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.4263 - dice_coef: 0.4263 - accuracy: 0.9980 - val_loss: -0.1995 - val_dice_coef: 0.1995 - val_accuracy: 0.9847\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.4317 - dice_coef: 0.4317 - accuracy: 0.9979 - val_loss: -0.1890 - val_dice_coef: 0.1890 - val_accuracy: 0.9858\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.4285 - dice_coef: 0.4285 - accuracy: 0.9971 - val_loss: -0.1847 - val_dice_coef: 0.1847 - val_accuracy: 0.9826\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.4178 - dice_coef: 0.4178 - accuracy: 0.9957 - val_loss: -0.1866 - val_dice_coef: 0.1866 - val_accuracy: 0.9792\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 52s 86ms/step - loss: -0.4269 - dice_coef: 0.4269 - accuracy: 0.9971 - val_loss: -0.1997 - val_dice_coef: 0.1997 - val_accuracy: 0.9834\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.4329 - dice_coef: 0.4329 - accuracy: 0.9978 - val_loss: -0.2059 - val_dice_coef: 0.2059 - val_accuracy: 0.9842\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.4381 - dice_coef: 0.4381 - accuracy: 0.9981 - val_loss: -0.1977 - val_dice_coef: 0.1977 - val_accuracy: 0.9853\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 51s 86ms/step - loss: -0.4375 - dice_coef: 0.4375 - accuracy: 0.9982 - val_loss: -0.2077 - val_dice_coef: 0.2077 - val_accuracy: 0.9850\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model = loaded_model.fit(X_train_new, Y_train, validation_data=(X_Validation_new,Y_Validation), batch_size=2, epochs=maxepoch, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "root_path = './training_results/Without_NAIP/'\n",
    "# save the trained model\n",
    "model_yaml = loaded_model.to_yaml()\n",
    "with open(root_path+\"model_fine_tuning_No_NAIP_1200_samples\"+timestr+\".yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "# save the weights\n",
    "loaded_model.save(root_path+\"model_fine_tuning_learning_No_NAIP_1200_samples\"+timestr+\".h5\")\n",
    "# save the intermdediate results and training statistics\n",
    "with open(root_path+\"history_fine_tuning_learning_No_NAIP_1200_samples\"+timestr+\".pickle\", 'wb') as file_pi:\n",
    "    pickle.dump(fine_tuned_model.history, file_pi, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progreess 10/12/2020\n",
    "\n",
    "Training from scratch   \n",
    "- Total params: 53,508,217  \n",
    "- Trainable params: 53,490,169  \n",
    "- Non-trainable params: 18,048  \n",
    "\n",
    "1. Traing the model from scratch without NAIP   \n",
    "        \n",
    "**Without NAIP**  \n",
    "The training takes 17 seconds for each epoch  \n",
    "- First training: the training stop at Epoch 105 (history_train_from_scratch_NoNAIP_20201012-111316)\n",
    "- We continue for 33 epochs more (history_train_from_scratch_NoNAIP_20201012-112416)\n",
    "    - Training set accu: 99.92%\n",
    "    - Validation set accu: 98.52%     \n",
    "\n",
    "**last line of log**   \n",
    "Epoch 33/400  \n",
    "50/50 [==============================] - 16s 313ms/step - loss: -0.9859 - dice_coef: 0.9859 - accuracy: 0.9992 - val_loss: -0.5574 - val_dice_coef: 0.5574 - val_accuracy: 0.9852\n",
    "\n",
    "1. Traing the model from scratch with NAIP   \n",
    "\n",
    "**With NAIP**   \n",
    "The training takes 17 seconds for each epoch \n",
    "- First training: the training stop at Epoch 141 (history_train_from_scratch_NAIP_20201012-121601)\n",
    "    - Training set accu: 99.91%\n",
    "    - Validation set accu: 98.87%   \n",
    "\n",
    "**last line of log**     \n",
    "Epoch 141/400   \n",
    "50/50 [==============================] - 16s 321ms/step - loss: -0.9849 - dice_coef: 0.9849 - accuracy: 0.9991 - val_loss: -0.6443 - val_dice_coef: 0.6443 - val_accuracy: 0.9887   \n",
    "  \n",
    "  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "  \n",
    "---- \n",
    "\n",
    "# Progress 28/09/2020    \n",
    "\n",
    "Established the baseline for **400 epochs**  \n",
    "                                                 \n",
    "Base line Model (Transfer learnign with NAIP data) can achieve validation acc. **92.89%**\n",
    "\n",
    "1. Added SpatialDropout2D layer    \n",
    "    **Result:** The dropout doesn't help in this case.     \n",
    "    with 0.3 drop rate and 400 epochs we can achieve validation acc. 92.63%   \n",
    "    with 0.5 drop rate and 400 epochs we can achieve validation acc. 92.32%  \n",
    "    with 0.7 drop rate and 400 epochs we can achieve validation acc. 92.06%   \n",
    "    \n",
    "    Compare to Dropout layer   \n",
    "    with 0.3 drop rate and 400 epochs we can achieve validation acc. 92.56%  \n",
    "    with 0.5 drop rate and 400 epochs we can achieve validation acc. 92.34%  \n",
    "    with 0.7 drop rate and 400 epochs we can achieve validation acc. 92.22%%  \n",
    "    \n",
    "2. Fine-tuning doesn't help much  \n",
    "    first pass learning rate 0.0001   \n",
    "    second pass learning rate 0.00001   \n",
    "    **Result:** we can achieve validation acc. 92.48%   \n",
    "   \n",
    "3. Woking on classifier after the convolutional network\n",
    "\n",
    "# Plan work\n",
    "1. Create colab notebook for RIF meeting \n",
    "2. Continue writing\n",
    "\n",
    "---\n",
    "\n",
    "# Progress 28/09/2020\n",
    "1. Added Dropout layer    \n",
    "    **Result:** The dropout doesn't help in this case.   \n",
    "    with 0.3 drop rate and 400 epochs we can achive 92.56%  \n",
    "    with 0.5 drop rate and 1000 epochs we can achive 94.1%  \n",
    "    with 0.7 drop rate and 400 epochs we can achive 92.2%  \n",
    "    \n",
    "2. Working on Fine-tuning process \n",
    "3. Working on Literature review\n",
    "\n",
    "\n",
    "# Plan work\n",
    "\n",
    "1. Finsihing the Literature review \n",
    "2. Try adding classifier after the convolutional network\n",
    "3. Begin the intro and abstract\n",
    "\n",
    "---\n",
    "\n",
    "# Progress 21/09/2020\n",
    "1. Genreating the result for transfer learnign without NAIP again \n",
    "2. Created the [plan until Mid Oct 2020](\"https://docs.google.com/document/d/1Kqz18zgB-DSkDarr-m8Y-__0ZCNzXAkTZw7Xg7kIy08/edit#\")\n",
    "    - The goal is to finish the first draft by Mid Oct. \n",
    "    \n",
    "# Plan work\n",
    "1. Start writing the paper\n",
    "2. Try training the model with more weight of stream class.\n",
    "3. Weekly plan until Mid October 2020[.]('https://analyticsindiamag.com/top-10-papers-on-transfer-learning-one-must-read-in-2020/')  \n",
    "\n",
    "--- \n",
    "\n",
    "# Progress 14/09/2020\n",
    "1. Read and summarize more  [transfer learning paper]('https://openreview.net/pdf?id=ryxyCeHtPB')  \n",
    "    - propose \"attentive feature distillation and selection (AFDS)\"   \n",
    "    - AFDS dynamically learns not only the features to transfer, but also the unimportant neurons to skip    \n",
    "    \n",
    "\n",
    "# Plan work\n",
    "1. Start writing the paper\n",
    "2. Try training the model with more weight of stream class.\n",
    "3. Weekly plan until Mid October 2020\n",
    "\n",
    "**Qual Exam beginning of next semester**\n",
    "\n",
    "---\n",
    "\n",
    "# Progress 07/09/2020  \n",
    "1. Presented the progress in CEGIS  \n",
    "  \n",
    "2. Generated total dataset for Covingtoin area (without NAIP imagery)\n",
    "    \n",
    "3. Run prediction of the Convington area with the dataset without NAIP and using the original model that is trained on Rowan creek area  \n",
    "\n",
    "\n",
    "# Plan work\n",
    "1. Try training the model with more weight of stream class.\n",
    "\n",
    "---\n",
    "\n",
    "# Progress 31/08/2020\n",
    "1. Corrected the data (removing None class (-9999) from test dataset)\n",
    "    - will generate the new test results  \n",
    "  \n",
    "  \n",
    "2. Preparing for CEGIS presentation\n",
    "    - Added prelim results  \n",
    "    - Will add the base scenario which is the U-net model predict the dataset without NAIP in Covinton river  \n",
    "      \n",
    "        \n",
    "    \n",
    "3. preparing the script for the presentation  \n",
    "    \n",
    "# Plan for this week\n",
    "1. Finish the presentation for CEGIS\n",
    "2. Read and summarize more paper\n",
    "3. Try training the model with more weight of stream class.\n",
    "\n",
    "----\n",
    "\n",
    "# Progress 24/08/2020\n",
    "\n",
    "**Comments:** Try to get the why and what it hold true and how to make or to apply to other places.  \n",
    "\n",
    "1. Generate the whole area and do testing\n",
    "    - Generated the dataset\n",
    "    - Evaluated the testing data and generated the prelim results\n",
    "**Problem:** the data has more than 2 classes as shown in evaluation.   \n",
    "      \n",
    "    \n",
    "2. Created the outline of the presentation for CEGIS \n",
    "    - Still need more details:   \n",
    "    https://docs.google.com/presentation/d/1PWrlgGEMCCJLXsAHeiTe40xdA22RtISE6gUpRbbplRs/edit?usp=sharing\n",
    "\n",
    "# Plan for this week\n",
    "1. Finish the presentation for CEGIS\n",
    "2. Read and summarize more paper\n",
    "3. Try training the model with more weight of stream class.\n",
    "4. Correct the data (remove the None class)\n",
    "\n",
    "---\n",
    "\n",
    "# Progress 17/08/2020\n",
    "1. Finished generating the new dataset\n",
    "    - Cleaned the NAIP data and all raw data of Covington River\n",
    "    - Included NAIP imagery into the dataset\n",
    "    - Edited the data preprocessing script to make it easier to add or remove data \n",
    "    - Added script documents and comments  \n",
    "      \n",
    "2. Generating the whole area dataset the included NAIP imagery\n",
    "    - Using High memory node on Keeling   \n",
    "    - **Problem:** The VPN disconnected after 2 hours in!!! T_T I have to start over.  \n",
    "  \n",
    "3. Trained the model with new dataset  \n",
    "    - The performance is significatly higher than the dataset without NAIP  \n",
    "  \n",
    "4. Read more papers and added summary of the read paper\n",
    "    -https://docs.google.com/document/d/1BApPn0aWTwstEpbnKC9g0p5KSOhi74_rF7nzRYM9CtE/edit  \n",
    "  \n",
    "# Plan for this week\n",
    "1. Generate the whole area and do testing\n",
    "2. Start preparing the presentation for CEGIS \n",
    "3. Read and summarize more papers\n",
    "    - Focus more on machine learning in hydro, remote sensing classification.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# Prgress 10/08/2020\n",
    "\n",
    "1. Successfully trained the model on my own PC.  \n",
    "    - Fixed cuCNN and CUDA version problems \n",
    "    - Trained with 4 trainable layers  \n",
    "      **Problem:** The model just disrtegards the stream class.  \n",
    "      **Root cause:** Unbalanced sample of stream and non-stream classes   \n",
    "\n",
    "2. In progress: Adding NAIP image to the dataset. \n",
    "    - Extracted the NAIP for Covinton and put it on Keeling \n",
    "    - modifying the preprocessing code\n",
    "    \n",
    "3. Outline the Introduction of the paper and reviewed some papers\n",
    "    - https://docs.google.com/document/d/1BApPn0aWTwstEpbnKC9g0p5KSOhi74_rF7nzRYM9CtE/edit\n",
    "    \n",
    "# Plan for this week\n",
    "1. Finished adding the NAIP and train the model again\n",
    "2. Start the first draft of the introduction \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
