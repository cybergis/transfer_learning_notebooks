{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T15:32:47.791942Z",
     "iopub.status.busy": "2020-10-12T15:32:47.791942Z",
     "iopub.status.idle": "2020-10-12T15:32:49.863350Z",
     "shell.execute_reply": "2020-10-12T15:32:49.862349Z",
     "shell.execute_reply.started": "2020-10-12T15:32:47.791942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "# Load all the dependencies\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from numpy import genfromtxt\n",
    "from tensorflow import random\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Layer, UpSampling2D, GlobalAveragePooling2D, Multiply, Dense, Reshape, Permute, multiply, dot, add, Input\n",
    "from keras.layers.core import Dropout, Lambda, SpatialDropout2D, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model, load_model, model_from_yaml, Sequential\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1337) # for reproducibility\n",
    "random.set_seed(1337)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T15:32:49.864351Z",
     "iopub.status.busy": "2020-10-12T15:32:49.864351Z",
     "iopub.status.idle": "2020-10-12T15:32:49.894379Z",
     "shell.execute_reply": "2020-10-12T15:32:49.894379Z",
     "shell.execute_reply.started": "2020-10-12T15:32:49.864351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use dice coefficient function as the loss function \n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
    "\n",
    "# Jacard coefficient\n",
    "def jacard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
    "\n",
    "# calculate loss value\n",
    "def jacard_coef_loss(y_true, y_pred):\n",
    "    return -jacard_coef(y_true, y_pred)\n",
    "\n",
    "# calculate loss value\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "def Residual_CNN_block(x, size, dropout=0.0, batch_norm=True):\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        axis = 1\n",
    "    else:\n",
    "        axis = 3\n",
    "    conv = Conv2D(size, (3, 3), padding='same')(x)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Conv2D(size, (3, 3), padding='same')(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Conv2D(size, (3, 3), padding='same')(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    return conv\n",
    "\n",
    "class multiplication(Layer):\n",
    "    def __init__(self,inter_channel = None,**kwargs):\n",
    "        super(multiplication, self).__init__(**kwargs)\n",
    "        self.inter_channel = inter_channel\n",
    "    def build(self,input_shape=None):\n",
    "        self.k = self.add_weight(name='k',shape=(1,),initializer='zeros',dtype='float32',trainable=True)\n",
    "    def get_config(self):\n",
    "        base_config = super(multiplication, self).get_config()\n",
    "        config = {'inter_channel':self.inter_channel}\n",
    "        return dict(list(base_config.items()) + list(config.items()))  \n",
    "    def call(self,inputs):\n",
    "        g,x,x_query,phi_g,x_value = inputs[0],inputs[1],inputs[2],inputs[3],inputs[4]\n",
    "        h,w,c = int(x.shape[1]),int(x.shape[2]),int(x.shape[3])\n",
    "        x_query = K.reshape(x_query, shape=(-1,h*w, self.inter_channel//4))\n",
    "        phi_g = K.reshape(phi_g,shape=(-1,h*w,self.inter_channel//4))\n",
    "        x_value = K.reshape(x_value,shape=(-1,h*w,c))\n",
    "        scale = dot([K.permute_dimensions(phi_g,(0,2,1)), x_query], axes=(1, 2))\n",
    "        soft_scale = Activation('softmax')(scale)\n",
    "        scaled_value = dot([K.permute_dimensions(soft_scale,(0,2,1)),K.permute_dimensions(x_value,(0,2,1))],axes=(1, 2))\n",
    "        scaled_value = K.reshape(scaled_value, shape=(-1,h,w,c))        \n",
    "        customize_multi = self.k * scaled_value\n",
    "        layero = add([customize_multi,x])\n",
    "        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n",
    "        concate = my_concat([layero,g])\n",
    "        return concate \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        ll = list(input_shape)[1]\n",
    "        return (None,ll[1],ll[1],ll[3]*3)\n",
    "    def get_custom_objects():\n",
    "        return {'multiplication': multiplication}\n",
    "\n",
    "def attention_up_and_concatenate(inputs):\n",
    "    g,x = inputs[0],inputs[1]\n",
    "    inter_channel = g.get_shape().as_list()[3]\n",
    "    g = Conv2DTranspose(inter_channel, (2,2), strides=[2, 2],padding='same')(g)\n",
    "    x_query = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(x)\n",
    "    phi_g = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(g)\n",
    "    x_value = Conv2D(inter_channel//2, [1, 1], strides=[1, 1], data_format='channels_last')(x)\n",
    "    inputs = [g,x,x_query,phi_g,x_value]\n",
    "    concate = multiplication(inter_channel)(inputs)\n",
    "    return concate\n",
    "\n",
    "class multiplication2(Layer):\n",
    "    def __init__(self,inter_channel = None,**kwargs):\n",
    "        super(multiplication2, self).__init__(**kwargs)\n",
    "        self.inter_channel = inter_channel\n",
    "    def build(self,input_shape=None):\n",
    "        self.k = self.add_weight(name='k',shape=(1,),initializer='zeros',dtype='float32',trainable=True)\n",
    "    def get_config(self):\n",
    "        base_config = super(multiplication2, self).get_config()\n",
    "        config = {'inter_channel':self.inter_channel}\n",
    "        return dict(list(base_config.items()) + list(config.items()))  \n",
    "    def call(self,inputs):\n",
    "        g,x,rate = inputs[0],inputs[1],inputs[2]\n",
    "        scaled_value = multiply([x, rate])\n",
    "        att_x =  self.k * scaled_value\n",
    "        att_x = add([att_x,x])\n",
    "        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n",
    "        concate = my_concat([att_x, g])\n",
    "        return concate \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        ll = list(input_shape)[1]\n",
    "        return (None,ll[1],ll[1],ll[3]*2)\n",
    "    def get_custom_objects():\n",
    "        return {'multiplication2': multiplication2}\n",
    "\n",
    "def attention_up_and_concatenate2(inputs):\n",
    "    g, x = inputs[0],inputs[1]\n",
    "    inter_channel = g.get_shape().as_list()[3]\n",
    "    g = Conv2DTranspose(inter_channel//2, (3,3), strides=[2, 2],padding='same')(g)\n",
    "    g = Conv2D(inter_channel//2, [1, 1], strides=[1, 1], data_format='channels_last')(g)\n",
    "    theta_x = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(x)\n",
    "    phi_g = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(g)\n",
    "    f = Activation('relu')(add([theta_x, phi_g]))\n",
    "    psi_f = Conv2D(1, [1, 1], strides=[1, 1], data_format='channels_last')(f)\n",
    "    rate = Activation('sigmoid')(psi_f)\n",
    "    concate =  multiplication2()([g,x,rate])\n",
    "    return concate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('June21/model/model_augv_attention2.h5', \n",
    "                             custom_objects={'multiplication': multiplication,'multiplication2': multiplication2, \n",
    "                                             'dice_coef_loss':dice_coef_loss, 'dice_coef':dice_coef,})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If want to train on the data **without** the NAIP, run the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 224, 224, 8)\n",
      "(600, 224, 224, 8)\n"
     ]
    }
   ],
   "source": [
    "# The dataset has 8 channels:\n",
    "# 0. Curvature\n",
    "# 1. Slope\n",
    "# 2. Openness\n",
    "# 3. DEM\n",
    "# 4. TPI 21\n",
    "# 5. Reflectance (LiDAR intensity)\n",
    "# 6. Geomorphon\n",
    "# 7. TPI 9\n",
    "# but the model expects 8 channels\n",
    "# So we exclude TPI_9 channel from the data set\n",
    "\n",
    "data_path = 'Covington_data/without_NAIP/'\n",
    "\n",
    "# read in training and validation sample patches\n",
    "X_train_new = np.load(data_path+'train_data.npy')\n",
    "X_Validation_new = np.load(data_path+'vali_data.npy')\n",
    "print(X_train_new.shape)\n",
    "print(X_Validation_new.shape)\n",
    "\n",
    "#Read training and validation labels\n",
    "Y_Validation = np.load(data_path+'vali_label.npy')\n",
    "Y_train = np.load(data_path+'train_label.npy')\n",
    "\n",
    "#Cast both labales to float32\n",
    "Y_Validation = Y_Validation.astype(np.float32)\n",
    "Y_train = Y_train.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 224\n",
    "IMG_WIDTH = patch_size\n",
    "IMG_HEIGHT = patch_size\n",
    "# Number of feature channels \n",
    "INPUT_CHANNELS = 8\n",
    "# Number of output masks (1 in case you predict only one type of objects)\n",
    "OUTPUT_MASK_CHANNELS = 1\n",
    "maxepoch = 1000\n",
    "# hyperparameters\n",
    "# learning_rate = 0.0000359\n",
    "learning_rate = 0.0001\n",
    "patience = 20\n",
    "aug = 'v'\n",
    "loaded_model.compile(optimizer=Adam(lr=learning_rate),loss = dice_coef_loss,metrics=[dice_coef,'accuracy'])\n",
    "callbacks = [\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=patience, min_lr=1e-9, verbose=1, mode='min'),\n",
    "        EarlyStopping(monitor='val_loss', patience=patience+10, verbose=0),\n",
    "        ModelCheckpoint('model'+aug+'_attention2.h5', monitor='val_loss', save_best_only=True, verbose=0),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " 2/50 [>.............................] - ETA: 5s - loss: -0.4932 - dice_coef: 0.4932 - accuracy: 0.9560WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0891s vs `on_train_batch_end` time: 0.1501s). Check your callbacks.\n",
      "50/50 [==============================] - ETA: 0s - loss: -0.6712 - dice_coef: 0.6712 - accuracy: 0.9773WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0110s vs `on_test_batch_end` time: 0.0601s). Check your callbacks.\n",
      "50/50 [==============================] - 17s 349ms/step - loss: -0.6712 - dice_coef: 0.6712 - accuracy: 0.9773 - val_loss: -0.5152 - val_dice_coef: 0.5152 - val_accuracy: 0.9741\n",
      "Epoch 2/1000\n",
      "50/50 [==============================] - 16s 325ms/step - loss: -0.7404 - dice_coef: 0.7404 - accuracy: 0.9829 - val_loss: -0.6798 - val_dice_coef: 0.6798 - val_accuracy: 0.9872\n",
      "Epoch 3/1000\n",
      "50/50 [==============================] - 15s 308ms/step - loss: -0.7662 - dice_coef: 0.7662 - accuracy: 0.9847 - val_loss: -0.6636 - val_dice_coef: 0.6636 - val_accuracy: 0.9867\n",
      "Epoch 4/1000\n",
      "50/50 [==============================] - 16s 327ms/step - loss: -0.7837 - dice_coef: 0.7837 - accuracy: 0.9857 - val_loss: -0.6971 - val_dice_coef: 0.6971 - val_accuracy: 0.9891\n",
      "Epoch 5/1000\n",
      "50/50 [==============================] - 16s 328ms/step - loss: -0.7935 - dice_coef: 0.7935 - accuracy: 0.9866 - val_loss: -0.7186 - val_dice_coef: 0.7186 - val_accuracy: 0.9888\n",
      "Epoch 6/1000\n",
      "50/50 [==============================] - 15s 309ms/step - loss: -0.8039 - dice_coef: 0.8039 - accuracy: 0.9877 - val_loss: -0.7100 - val_dice_coef: 0.7100 - val_accuracy: 0.9909\n",
      "Epoch 7/1000\n",
      "50/50 [==============================] - 16s 328ms/step - loss: -0.8332 - dice_coef: 0.8332 - accuracy: 0.9890 - val_loss: -0.7457 - val_dice_coef: 0.7457 - val_accuracy: 0.9915\n",
      "Epoch 8/1000\n",
      "50/50 [==============================] - 16s 328ms/step - loss: -0.8442 - dice_coef: 0.8442 - accuracy: 0.9900 - val_loss: -0.7672 - val_dice_coef: 0.7672 - val_accuracy: 0.9921\n",
      "Epoch 9/1000\n",
      "50/50 [==============================] - 15s 310ms/step - loss: -0.8463 - dice_coef: 0.8463 - accuracy: 0.9900 - val_loss: -0.7442 - val_dice_coef: 0.7442 - val_accuracy: 0.9907\n",
      "Epoch 10/1000\n",
      "50/50 [==============================] - 16s 310ms/step - loss: -0.8475 - dice_coef: 0.8475 - accuracy: 0.9901 - val_loss: -0.7251 - val_dice_coef: 0.7251 - val_accuracy: 0.9909\n",
      "Epoch 11/1000\n",
      "50/50 [==============================] - 16s 310ms/step - loss: -0.8437 - dice_coef: 0.8437 - accuracy: 0.9898 - val_loss: -0.7164 - val_dice_coef: 0.7164 - val_accuracy: 0.9890\n",
      "Epoch 12/1000\n",
      "50/50 [==============================] - 16s 310ms/step - loss: -0.8695 - dice_coef: 0.8695 - accuracy: 0.9915 - val_loss: -0.7629 - val_dice_coef: 0.7629 - val_accuracy: 0.9923\n",
      "Epoch 13/1000\n",
      "50/50 [==============================] - 16s 310ms/step - loss: -0.8767 - dice_coef: 0.8767 - accuracy: 0.9920 - val_loss: -0.7546 - val_dice_coef: 0.7546 - val_accuracy: 0.9918\n",
      "Epoch 14/1000\n",
      "50/50 [==============================] - 16s 329ms/step - loss: -0.8816 - dice_coef: 0.8816 - accuracy: 0.9921 - val_loss: -0.7705 - val_dice_coef: 0.7705 - val_accuracy: 0.9921\n",
      "Epoch 15/1000\n",
      "50/50 [==============================] - 16s 310ms/step - loss: -0.8846 - dice_coef: 0.8846 - accuracy: 0.9923 - val_loss: -0.6989 - val_dice_coef: 0.6989 - val_accuracy: 0.9910\n",
      "Epoch 16/1000\n",
      "50/50 [==============================] - 16s 311ms/step - loss: -0.8902 - dice_coef: 0.8902 - accuracy: 0.9929 - val_loss: -0.7381 - val_dice_coef: 0.7381 - val_accuracy: 0.9903\n",
      "Epoch 17/1000\n",
      "50/50 [==============================] - 16s 313ms/step - loss: -0.8814 - dice_coef: 0.8814 - accuracy: 0.9924 - val_loss: -0.7545 - val_dice_coef: 0.7545 - val_accuracy: 0.9913\n",
      "Epoch 18/1000\n",
      "50/50 [==============================] - 16s 315ms/step - loss: -0.9048 - dice_coef: 0.9048 - accuracy: 0.9938 - val_loss: -0.7469 - val_dice_coef: 0.7469 - val_accuracy: 0.9916\n",
      "Epoch 19/1000\n",
      "50/50 [==============================] - 16s 310ms/step - loss: -0.9120 - dice_coef: 0.9120 - accuracy: 0.9942 - val_loss: -0.7682 - val_dice_coef: 0.7682 - val_accuracy: 0.9919\n",
      "Epoch 20/1000\n",
      "50/50 [==============================] - 16s 310ms/step - loss: -0.9138 - dice_coef: 0.9138 - accuracy: 0.9943 - val_loss: -0.7594 - val_dice_coef: 0.7594 - val_accuracy: 0.9915\n",
      "Epoch 21/1000\n",
      "50/50 [==============================] - 16s 329ms/step - loss: -0.9191 - dice_coef: 0.9191 - accuracy: 0.9946 - val_loss: -0.7754 - val_dice_coef: 0.7754 - val_accuracy: 0.9922\n",
      "Epoch 22/1000\n",
      "50/50 [==============================] - 16s 310ms/step - loss: -0.9276 - dice_coef: 0.9276 - accuracy: 0.9951 - val_loss: -0.7542 - val_dice_coef: 0.7542 - val_accuracy: 0.9919\n",
      "Epoch 23/1000\n",
      "50/50 [==============================] - 16s 310ms/step - loss: -0.9308 - dice_coef: 0.9308 - accuracy: 0.9955 - val_loss: -0.7668 - val_dice_coef: 0.7668 - val_accuracy: 0.9919\n",
      "Epoch 24/1000\n",
      "50/50 [==============================] - 16s 310ms/step - loss: -0.9293 - dice_coef: 0.9293 - accuracy: 0.9953 - val_loss: -0.7728 - val_dice_coef: 0.7728 - val_accuracy: 0.9923\n",
      "Epoch 25/1000\n",
      "50/50 [==============================] - 16s 310ms/step - loss: -0.9313 - dice_coef: 0.9313 - accuracy: 0.9954 - val_loss: -0.7393 - val_dice_coef: 0.7393 - val_accuracy: 0.9916\n",
      "Epoch 26/1000\n",
      "50/50 [==============================] - 16s 310ms/step - loss: -0.9336 - dice_coef: 0.9336 - accuracy: 0.9956 - val_loss: -0.7585 - val_dice_coef: 0.7585 - val_accuracy: 0.9910\n",
      "Epoch 27/1000\n",
      "50/50 [==============================] - 16s 310ms/step - loss: -0.9388 - dice_coef: 0.9388 - accuracy: 0.9959 - val_loss: -0.7195 - val_dice_coef: 0.7195 - val_accuracy: 0.9913\n",
      "Epoch 28/1000\n",
      "50/50 [==============================] - 16s 311ms/step - loss: -0.9345 - dice_coef: 0.9345 - accuracy: 0.9957 - val_loss: -0.7427 - val_dice_coef: 0.7427 - val_accuracy: 0.9916\n",
      "Epoch 29/1000\n",
      "50/50 [==============================] - 16s 311ms/step - loss: -0.9406 - dice_coef: 0.9406 - accuracy: 0.9961 - val_loss: -0.7393 - val_dice_coef: 0.7393 - val_accuracy: 0.9915\n",
      "Epoch 30/1000\n",
      "50/50 [==============================] - 16s 329ms/step - loss: -0.9393 - dice_coef: 0.9393 - accuracy: 0.9961 - val_loss: -0.7758 - val_dice_coef: 0.7758 - val_accuracy: 0.9915\n",
      "Epoch 31/1000\n",
      "50/50 [==============================] - 16s 310ms/step - loss: -0.9385 - dice_coef: 0.9385 - accuracy: 0.9958 - val_loss: -0.7622 - val_dice_coef: 0.7622 - val_accuracy: 0.9919\n",
      "Epoch 32/1000\n",
      "50/50 [==============================] - 15s 310ms/step - loss: -0.9424 - dice_coef: 0.9424 - accuracy: 0.9962 - val_loss: -0.7505 - val_dice_coef: 0.7505 - val_accuracy: 0.9919\n",
      "Epoch 33/1000\n",
      "50/50 [==============================] - 16s 310ms/step - loss: -0.9439 - dice_coef: 0.9439 - accuracy: 0.9962 - val_loss: -0.7483 - val_dice_coef: 0.7483 - val_accuracy: 0.9912\n",
      "Epoch 34/1000\n",
      "50/50 [==============================] - 15s 310ms/step - loss: -0.9438 - dice_coef: 0.9438 - accuracy: 0.9963 - val_loss: -0.7558 - val_dice_coef: 0.7558 - val_accuracy: 0.9919\n",
      "Epoch 35/1000\n",
      "50/50 [==============================] - 16s 328ms/step - loss: -0.9460 - dice_coef: 0.9460 - accuracy: 0.9963 - val_loss: -0.7763 - val_dice_coef: 0.7763 - val_accuracy: 0.9923\n",
      "Epoch 36/1000\n",
      "50/50 [==============================] - 15s 310ms/step - loss: -0.9446 - dice_coef: 0.9446 - accuracy: 0.9962 - val_loss: -0.7625 - val_dice_coef: 0.7625 - val_accuracy: 0.9916\n",
      "Epoch 37/1000\n",
      "50/50 [==============================] - 15s 310ms/step - loss: -0.9436 - dice_coef: 0.9436 - accuracy: 0.9963 - val_loss: -0.7602 - val_dice_coef: 0.7602 - val_accuracy: 0.9917\n",
      "Epoch 38/1000\n",
      "50/50 [==============================] - 15s 309ms/step - loss: -0.7758 - dice_coef: 0.7758 - accuracy: 0.9729 - val_loss: -0.0079 - val_dice_coef: 0.0079 - val_accuracy: 0.9811\n",
      "Epoch 39/1000\n",
      "50/50 [==============================] - 15s 308ms/step - loss: -0.1205 - dice_coef: 0.1205 - accuracy: 0.8902 - val_loss: -0.0114 - val_dice_coef: 0.0114 - val_accuracy: 0.9811\n",
      "Epoch 40/1000\n",
      "50/50 [==============================] - 16s 310ms/step - loss: -0.1265 - dice_coef: 0.1265 - accuracy: 0.9103 - val_loss: -0.0167 - val_dice_coef: 0.0167 - val_accuracy: 0.9811\n",
      "Epoch 41/1000\n",
      "50/50 [==============================] - 15s 309ms/step - loss: -0.1265 - dice_coef: 0.1265 - accuracy: 0.9090 - val_loss: -0.0159 - val_dice_coef: 0.0159 - val_accuracy: 0.9811\n",
      "Epoch 42/1000\n",
      "50/50 [==============================] - 15s 309ms/step - loss: -0.1272 - dice_coef: 0.1272 - accuracy: 0.9099 - val_loss: -0.0154 - val_dice_coef: 0.0154 - val_accuracy: 0.9811\n",
      "Epoch 43/1000\n",
      "50/50 [==============================] - 15s 309ms/step - loss: -0.1264 - dice_coef: 0.1264 - accuracy: 0.9105 - val_loss: -0.0141 - val_dice_coef: 0.0141 - val_accuracy: 0.9811\n",
      "Epoch 44/1000\n",
      "50/50 [==============================] - 15s 309ms/step - loss: -0.1269 - dice_coef: 0.1269 - accuracy: 0.9134 - val_loss: -0.0151 - val_dice_coef: 0.0151 - val_accuracy: 0.9811\n",
      "Epoch 45/1000\n",
      "50/50 [==============================] - 15s 309ms/step - loss: -0.1278 - dice_coef: 0.1278 - accuracy: 0.9059 - val_loss: -0.0164 - val_dice_coef: 0.0164 - val_accuracy: 0.9811\n",
      "Epoch 46/1000\n",
      "50/50 [==============================] - 15s 308ms/step - loss: -0.1280 - dice_coef: 0.1280 - accuracy: 0.9127 - val_loss: -0.0171 - val_dice_coef: 0.0171 - val_accuracy: 0.9811\n",
      "Epoch 47/1000\n",
      "50/50 [==============================] - 15s 309ms/step - loss: -0.1274 - dice_coef: 0.1274 - accuracy: 0.9147 - val_loss: -0.0168 - val_dice_coef: 0.0168 - val_accuracy: 0.9811\n",
      "Epoch 48/1000\n",
      "50/50 [==============================] - 15s 309ms/step - loss: -0.1283 - dice_coef: 0.1283 - accuracy: 0.9125 - val_loss: -0.0155 - val_dice_coef: 0.0155 - val_accuracy: 0.9811\n",
      "Epoch 49/1000\n",
      "50/50 [==============================] - 15s 308ms/step - loss: -0.1282 - dice_coef: 0.1282 - accuracy: 0.9090 - val_loss: -0.0154 - val_dice_coef: 0.0154 - val_accuracy: 0.9811\n",
      "Epoch 50/1000\n",
      "50/50 [==============================] - 15s 309ms/step - loss: -0.1284 - dice_coef: 0.1284 - accuracy: 0.9123 - val_loss: -0.0129 - val_dice_coef: 0.0129 - val_accuracy: 0.9811\n",
      "Epoch 51/1000\n",
      "50/50 [==============================] - 15s 309ms/step - loss: -0.1267 - dice_coef: 0.1267 - accuracy: 0.9056 - val_loss: -0.0096 - val_dice_coef: 0.0096 - val_accuracy: 0.9811\n",
      "Epoch 52/1000\n",
      "50/50 [==============================] - 15s 308ms/step - loss: -0.1283 - dice_coef: 0.1283 - accuracy: 0.9087 - val_loss: -0.0079 - val_dice_coef: 0.0079 - val_accuracy: 0.9811\n",
      "Epoch 53/1000\n",
      "50/50 [==============================] - 15s 308ms/step - loss: -0.1278 - dice_coef: 0.1278 - accuracy: 0.9098 - val_loss: -0.0078 - val_dice_coef: 0.0078 - val_accuracy: 0.9811\n",
      "Epoch 54/1000\n",
      "50/50 [==============================] - 15s 308ms/step - loss: -0.1294 - dice_coef: 0.1294 - accuracy: 0.9111 - val_loss: -0.0071 - val_dice_coef: 0.0071 - val_accuracy: 0.9811\n",
      "Epoch 55/1000\n",
      "50/50 [==============================] - ETA: 0s - loss: -0.1296 - dice_coef: 0.1296 - accuracy: 0.9088\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "50/50 [==============================] - 15s 309ms/step - loss: -0.1296 - dice_coef: 0.1296 - accuracy: 0.9088 - val_loss: -0.0064 - val_dice_coef: 0.0064 - val_accuracy: 0.9811\n",
      "Epoch 56/1000\n",
      "50/50 [==============================] - 15s 309ms/step - loss: -0.1293 - dice_coef: 0.1293 - accuracy: 0.9084 - val_loss: -0.0062 - val_dice_coef: 0.0062 - val_accuracy: 0.9811\n",
      "Epoch 57/1000\n",
      "50/50 [==============================] - 15s 308ms/step - loss: -0.1301 - dice_coef: 0.1301 - accuracy: 0.9114 - val_loss: -0.0067 - val_dice_coef: 0.0067 - val_accuracy: 0.9811\n",
      "Epoch 58/1000\n",
      "50/50 [==============================] - 15s 308ms/step - loss: -0.1301 - dice_coef: 0.1301 - accuracy: 0.9107 - val_loss: -0.0070 - val_dice_coef: 0.0070 - val_accuracy: 0.9811\n",
      "Epoch 59/1000\n",
      "50/50 [==============================] - 15s 308ms/step - loss: -0.1302 - dice_coef: 0.1302 - accuracy: 0.9102 - val_loss: -0.0069 - val_dice_coef: 0.0069 - val_accuracy: 0.9811\n",
      "Epoch 60/1000\n",
      "50/50 [==============================] - 15s 308ms/step - loss: -0.1307 - dice_coef: 0.1307 - accuracy: 0.9105 - val_loss: -0.0070 - val_dice_coef: 0.0070 - val_accuracy: 0.9811\n",
      "Epoch 61/1000\n",
      "50/50 [==============================] - 15s 308ms/step - loss: -0.1298 - dice_coef: 0.1298 - accuracy: 0.9094 - val_loss: -0.0069 - val_dice_coef: 0.0069 - val_accuracy: 0.9811\n",
      "Epoch 62/1000\n",
      "50/50 [==============================] - 15s 308ms/step - loss: -0.1279 - dice_coef: 0.1279 - accuracy: 0.9125 - val_loss: -0.0070 - val_dice_coef: 0.0070 - val_accuracy: 0.9811\n",
      "Epoch 63/1000\n",
      "50/50 [==============================] - 15s 308ms/step - loss: -0.1291 - dice_coef: 0.1291 - accuracy: 0.9097 - val_loss: -0.0071 - val_dice_coef: 0.0071 - val_accuracy: 0.9811\n",
      "Epoch 64/1000\n",
      "50/50 [==============================] - 15s 308ms/step - loss: -0.1303 - dice_coef: 0.1303 - accuracy: 0.9119 - val_loss: -0.0073 - val_dice_coef: 0.0073 - val_accuracy: 0.9811\n",
      "Epoch 65/1000\n",
      "50/50 [==============================] - 15s 308ms/step - loss: -0.1309 - dice_coef: 0.1309 - accuracy: 0.9112 - val_loss: -0.0071 - val_dice_coef: 0.0071 - val_accuracy: 0.9811\n"
     ]
    }
   ],
   "source": [
    "no_transfer_learning_results = loaded_model.fit(X_train_new, Y_train, validation_data=(X_Validation_new,Y_Validation), batch_size=12, epochs=maxepoch, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "root_path = ''\n",
    "# save the trained model\n",
    "model_yaml = loaded_model.to_yaml()\n",
    "with open(root_path+\"model_attention2_Notransfered_NoNAIP_\"+timestr+\".yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "# save the weights\n",
    "loaded_model.save(root_path+\"model_attention2_Notransfered_NoNAIP_\"+timestr+\".h5\")\n",
    "# save the intermdediate results and training statistics\n",
    "with open(root_path+\"history_attention2_Notransfered_NoNAIP_\"+timestr+\".pickle\", 'wb') as file_pi:\n",
    "    pickle.dump(no_transfer_learning_results.history, file_pi, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will wait for the whole area data to do prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predicted labels.\n",
    "X_test = np.load('Covington_data/without_NAIP/prediction_data_07092020.npy')\n",
    "preds_test = loaded_model.predict(X_test)\n",
    "preds_test_t = (preds_test > 0.5).astype(np.uint8)\n",
    "np.save('Covington_data/without_NAIP/pred_Rowan_mdl_Covington_data.npy',preds_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress 31/08/2020\n",
    "# Progress 31/08/2020\n",
    "1. Corrected the data (removing None class (-9999) from test dataset)\n",
    "    - will generate the new test results  \n",
    "  \n",
    "  \n",
    "2. Preparing for CEGIS presentation\n",
    "    - Added prelim results  \n",
    "    - Will add the base scenario which is the U-net model predict the dataset without NAIP in Covinton river  \n",
    "      \n",
    "        \n",
    "    \n",
    "3. preparing the script for the presentation  \n",
    "    \n",
    "# Plan for this week\n",
    "1. Finish the presentation for CEGIS\n",
    "2. Read and summarize more paper\n",
    "3. Try training the model with more weight of stream class.\n",
    "\n",
    "----\n",
    "\n",
    "# Progress 24/08/2020\n",
    "\n",
    "**Comments:** Try to get the why and what it hold true and how to make or to apply to other places.  \n",
    "\n",
    "1. Generate the whole area and do testing\n",
    "    - Generated the dataset\n",
    "    - Evaluated the testing data and generated the prelim results\n",
    "**Problem:** the data has more than 2 classes as shown in evaluation.   \n",
    "      \n",
    "    \n",
    "2. Created the outline of the presentation for CEGIS \n",
    "    - Still need more details:   \n",
    "    https://docs.google.com/presentation/d/1PWrlgGEMCCJLXsAHeiTe40xdA22RtISE6gUpRbbplRs/edit?usp=sharing\n",
    "\n",
    "# Plan for this week\n",
    "1. Finish the presentation for CEGIS\n",
    "2. Read and summarize more paper\n",
    "3. Try training the model with more weight of stream class.\n",
    "4. Correct the data (remove the None class)\n",
    "\n",
    "---\n",
    "\n",
    "# Progress 17/08/2020\n",
    "1. Finished generating the new dataset\n",
    "    - Cleaned the NAIP data and all raw data of Covington River\n",
    "    - Included NAIP imagery into the dataset\n",
    "    - Edited the data preprocessing script to make it easier to add or remove data \n",
    "    - Added script documents and comments  \n",
    "      \n",
    "2. Generating the whole area dataset the included NAIP imagery\n",
    "    - Using High memory node on Keeling   \n",
    "    - **Problem:** The VPN disconnected after 2 hours in!!! T_T I have to start over.  \n",
    "  \n",
    "3. Trained the model with new dataset  \n",
    "    - The performance is significatly higher than the dataset without NAIP  \n",
    "  \n",
    "4. Read more papers and added summary of the read paper\n",
    "    -https://docs.google.com/document/d/1BApPn0aWTwstEpbnKC9g0p5KSOhi74_rF7nzRYM9CtE/edit  \n",
    "  \n",
    "# Plan for this week\n",
    "1. Generate the whole area and do testing\n",
    "2. Start preparing the presentation for CEGIS \n",
    "3. Read and summarize more papers\n",
    "    - Focus more on machine learning in hydro, remote sensing classification.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# Prgress 10/08/2020\n",
    "\n",
    "1. Successfully trained the model on my own PC.  \n",
    "    - Fixed cuCNN and CUDA version problems \n",
    "    - Trained with 4 trainable layers  \n",
    "      **Problem:** The model just disrtegards the stream class.  \n",
    "      **Root cause:** Unbalanced sample of stream and non-stream classes   \n",
    "\n",
    "2. In progress: Adding NAIP image to the dataset. \n",
    "    - Extracted the NAIP for Covinton and put it on Keeling \n",
    "    - modifying the preprocessing code\n",
    "    \n",
    "3. Outline the Introduction of the paper and reviewed some papers\n",
    "    - https://docs.google.com/document/d/1BApPn0aWTwstEpbnKC9g0p5KSOhi74_rF7nzRYM9CtE/edit\n",
    "    \n",
    "# Plan for this week\n",
    "1. Finished adding the NAIP and train the model again\n",
    "2. Start the first draft of the introduction \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
