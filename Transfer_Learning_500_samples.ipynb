{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Load all the dependencies\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from numpy import genfromtxt\n",
    "from tensorflow import random\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Layer, UpSampling2D, GlobalAveragePooling2D, Multiply, Dense, Reshape, Permute, multiply, dot, add, Input\n",
    "from keras.layers.core import Dropout, Lambda, SpatialDropout2D, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model, load_model, model_from_yaml, Sequential\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1337) # for reproducibility\n",
    "random.set_seed(1337)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dice coefficient function as the loss function \n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
    "\n",
    "# Jacard coefficient\n",
    "def jacard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
    "\n",
    "# calculate loss value\n",
    "def jacard_coef_loss(y_true, y_pred):\n",
    "    return -jacard_coef(y_true, y_pred)\n",
    "\n",
    "# calculate loss value\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "def Residual_CNN_block(x, size, dropout=0.0, batch_norm=True):\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        axis = 1\n",
    "    else:\n",
    "        axis = 3\n",
    "    conv = Conv2D(size, (3, 3), padding='same')(x)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Conv2D(size, (3, 3), padding='same')(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Conv2D(size, (3, 3), padding='same')(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    return conv\n",
    "\n",
    "class multiplication(Layer):\n",
    "    def __init__(self,inter_channel = None,**kwargs):\n",
    "        super(multiplication, self).__init__(**kwargs)\n",
    "        self.inter_channel = inter_channel\n",
    "    def build(self,input_shape=None):\n",
    "        self.k = self.add_weight(name='k',shape=(1,),initializer='zeros',dtype='float32',trainable=True)\n",
    "    def get_config(self):\n",
    "        base_config = super(multiplication, self).get_config()\n",
    "        config = {'inter_channel':self.inter_channel}\n",
    "        return dict(list(base_config.items()) + list(config.items()))  \n",
    "    def call(self,inputs):\n",
    "        g,x,x_query,phi_g,x_value = inputs[0],inputs[1],inputs[2],inputs[3],inputs[4]\n",
    "        h,w,c = int(x.shape[1]),int(x.shape[2]),int(x.shape[3])\n",
    "        x_query = K.reshape(x_query, shape=(-1,h*w, self.inter_channel//4))\n",
    "        phi_g = K.reshape(phi_g,shape=(-1,h*w,self.inter_channel//4))\n",
    "        x_value = K.reshape(x_value,shape=(-1,h*w,c))\n",
    "        scale = dot([K.permute_dimensions(phi_g,(0,2,1)), x_query], axes=(1, 2))\n",
    "        soft_scale = Activation('softmax')(scale)\n",
    "        scaled_value = dot([K.permute_dimensions(soft_scale,(0,2,1)),K.permute_dimensions(x_value,(0,2,1))],axes=(1, 2))\n",
    "        scaled_value = K.reshape(scaled_value, shape=(-1,h,w,c))        \n",
    "        customize_multi = self.k * scaled_value\n",
    "        layero = add([customize_multi,x])\n",
    "        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n",
    "        concate = my_concat([layero,g])\n",
    "        return concate \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        ll = list(input_shape)[1]\n",
    "        return (None,ll[1],ll[1],ll[3]*3)\n",
    "    def get_custom_objects():\n",
    "        return {'multiplication': multiplication}\n",
    "\n",
    "def attention_up_and_concatenate(inputs):\n",
    "    g,x = inputs[0],inputs[1]\n",
    "    inter_channel = g.get_shape().as_list()[3]\n",
    "    g = Conv2DTranspose(inter_channel, (2,2), strides=[2, 2],padding='same')(g)\n",
    "    x_query = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(x)\n",
    "    phi_g = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(g)\n",
    "    x_value = Conv2D(inter_channel//2, [1, 1], strides=[1, 1], data_format='channels_last')(x)\n",
    "    inputs = [g,x,x_query,phi_g,x_value]\n",
    "    concate = multiplication(inter_channel)(inputs)\n",
    "    return concate\n",
    "\n",
    "class multiplication2(Layer):\n",
    "    def __init__(self,inter_channel = None,**kwargs):\n",
    "        super(multiplication2, self).__init__(**kwargs)\n",
    "        self.inter_channel = inter_channel\n",
    "    def build(self,input_shape=None):\n",
    "        self.k = self.add_weight(name='k',shape=(1,),initializer='zeros',dtype='float32',trainable=True)\n",
    "    def get_config(self):\n",
    "        base_config = super(multiplication2, self).get_config()\n",
    "        config = {'inter_channel':self.inter_channel}\n",
    "        return dict(list(base_config.items()) + list(config.items()))  \n",
    "    def call(self,inputs):\n",
    "        g,x,rate = inputs[0],inputs[1],inputs[2]\n",
    "        scaled_value = multiply([x, rate])\n",
    "        att_x =  self.k * scaled_value\n",
    "        att_x = add([att_x,x])\n",
    "        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n",
    "        concate = my_concat([att_x, g])\n",
    "        return concate \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        ll = list(input_shape)[1]\n",
    "        return (None,ll[1],ll[1],ll[3]*2)\n",
    "    def get_custom_objects():\n",
    "        return {'multiplication2': multiplication2}\n",
    "\n",
    "def attention_up_and_concatenate2(inputs):\n",
    "    g, x = inputs[0],inputs[1]\n",
    "    inter_channel = g.get_shape().as_list()[3]\n",
    "    g = Conv2DTranspose(inter_channel//2, (3,3), strides=[2, 2],padding='same')(g)\n",
    "    g = Conv2D(inter_channel//2, [1, 1], strides=[1, 1], data_format='channels_last')(g)\n",
    "    theta_x = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(x)\n",
    "    phi_g = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(g)\n",
    "    f = Activation('relu')(add([theta_x, phi_g]))\n",
    "    psi_f = Conv2D(1, [1, 1], strides=[1, 1], data_format='channels_last')(f)\n",
    "    rate = Activation('sigmoid')(psi_f)\n",
    "    concate =  multiplication2()([g,x,rate])\n",
    "    return concate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "Here\n",
      "Here\n",
      "Here\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('June21/model/model_augv_attention2.h5', \n",
    "                             custom_objects={'multiplication': multiplication,'multiplication2': multiplication2, \n",
    "                                             'dice_coef_loss':dice_coef_loss, 'dice_coef':dice_coef,})\n",
    "\n",
    "# remove the last 2 layer using pop() function\n",
    "loaded_model.layers.pop()\n",
    "loaded_model.layers.pop()\n",
    "\n",
    "for (index, layer) in enumerate(loaded_model.layers):\n",
    "    if (index > len(loaded_model.layers)-5):\n",
    "      print(\"Here\")\n",
    "      layer.trainable = True\n",
    "    else:\n",
    "      layer.trainable = False\n",
    "\n",
    "# Create new model from the model using the input and output of the last layer (after poping last 2 layers)\n",
    "model_without_last = Model(loaded_model.input,  loaded_model.layers[-1].output)\n",
    "\n",
    "# See model structure\n",
    "#model_without_last.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of output masks (1 in case you predict only one type of objects)\n",
    "OUTPUT_MASK_CHANNELS = 1\n",
    "\n",
    "# 1 dimensional convolution and generate probabilities from Sigmoid function\n",
    "conv_final = Conv2D(OUTPUT_MASK_CHANNELS, (1, 1), name='conv2d_last')(model_without_last.output)\n",
    "new_out = Activation('sigmoid', name='activation_last')(conv_final)\n",
    "\n",
    "# Created new model with the newly added last two layers \n",
    "transfered_model = Model(inputs=model_without_last.input, outputs=new_out)\n",
    "\n",
    "# New model structure\n",
    "#transfered_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If want to train on the data **without** the NAIP, run the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350, 224, 224, 8)\n",
      "(400, 224, 224, 8)\n"
     ]
    }
   ],
   "source": [
    "data_path = 'Covington_data/non-overlap/'\n",
    "\n",
    "# read in training and validation data\n",
    "X_train = np.load(data_path+'train_data.npy')\n",
    "Y_train = np.load(data_path+'train_label.npy')\n",
    "X_Validation = np.load(data_path+'vali_data.npy')\n",
    "Y_Validation = np.load(data_path+'vali_label.npy')\n",
    "\n",
    "# The dataset has 9 channels:\n",
    "# 0. Curvature\n",
    "# 1. Slope\n",
    "# 2. Openness\n",
    "# 3. DEM\n",
    "# 4. TPI 21\n",
    "# 5. Reflectance (LiDAR intensity)\n",
    "# 6. Geomorphon\n",
    "# 7. TPI 9\n",
    "# 8. TPI 3\n",
    "# but the model expects 8 channels\n",
    "# So we exclude TPI_9 channel from the data set\n",
    "X_train_new = X_train[:,:,:,(0,1,2,3,4,5,6,8)]\n",
    "print(X_train_new.shape)\n",
    "\n",
    "X_Validation_new = X_Validation[:,:,:,(0,1,2,3,4,5,6,8)]\n",
    "print(X_Validation_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If want to train on the data **with** the NAIP, run the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T18:54:22.204136Z",
     "iopub.status.busy": "2020-10-12T18:54:22.204136Z",
     "iopub.status.idle": "2020-10-12T18:54:22.500927Z",
     "shell.execute_reply": "2020-10-12T18:54:22.500927Z",
     "shell.execute_reply.started": "2020-10-12T18:54:22.204136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 224, 224, 8)\n",
      "(40, 224, 224, 8)\n"
     ]
    }
   ],
   "source": [
    "data_path = 'Covington_data/include_NAIP/nodata_as_0/500_samples/'\n",
    "\n",
    "# read in training and validation sample patches\n",
    "X_train_new = np.load(data_path+'train_data.npy')\n",
    "X_Validation_new = np.load(data_path+'vali_data.npy')\n",
    "print(X_train_new.shape)\n",
    "print(X_Validation_new.shape)\n",
    "\n",
    "#Read training and validation labels\n",
    "Y_Validation = np.load(data_path+'vali_label.npy')\n",
    "Y_train = np.load(data_path+'train_label.npy')\n",
    "\n",
    "#Cast both labales to float32\n",
    "Y_Validation = Y_Validation.astype(np.float32)\n",
    "Y_train = Y_train.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T18:54:32.764277Z",
     "iopub.status.busy": "2020-10-12T18:54:32.763276Z",
     "iopub.status.idle": "2020-10-12T18:54:32.786850Z",
     "shell.execute_reply": "2020-10-12T18:54:32.786850Z",
     "shell.execute_reply.started": "2020-10-12T18:54:32.764277Z"
    }
   },
   "outputs": [],
   "source": [
    "patch_size = 224\n",
    "IMG_WIDTH = patch_size\n",
    "IMG_HEIGHT = patch_size\n",
    "# Number of feature channels \n",
    "INPUT_CHANNELS = 8\n",
    "# Number of output masks (1 in case you predict only one type of objects)\n",
    "OUTPUT_MASK_CHANNELS = 1\n",
    "maxepoch = 1000\n",
    "# hyperparameters\n",
    "# learning_rate = 0.0000359\n",
    "learning_rate = 0.0001\n",
    "patience = 20\n",
    "aug = 'v'\n",
    "transfered_model.compile(optimizer=Adam(lr=learning_rate),loss = dice_coef_loss,metrics=[dice_coef,'accuracy'])\n",
    "callbacks = [\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=patience, min_lr=1e-9, verbose=1, mode='min'),\n",
    "        EarlyStopping(monitor='val_loss', patience=patience+10, verbose=0),\n",
    "        ModelCheckpoint('model'+aug+'_attention2.h5', monitor='val_loss', save_best_only=True, verbose=0),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T18:55:33.726517Z",
     "iopub.status.busy": "2020-10-12T18:55:33.726517Z",
     "iopub.status.idle": "2020-10-12T18:55:38.737682Z",
     "shell.execute_reply": "2020-10-12T18:55:38.736682Z",
     "shell.execute_reply.started": "2020-10-12T18:55:33.726517Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node functional_3/conv2d_1/Conv2D (defined at <ipython-input-7-258e52ca8d19>:1) ]] [Op:__inference_train_function_12949]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-258e52ca8d19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mltranfer_learning_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransfered_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_Validation_new\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_Validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node functional_3/conv2d_1/Conv2D (defined at <ipython-input-7-258e52ca8d19>:1) ]] [Op:__inference_train_function_12949]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "ltranfer_learning_results = transfered_model.fit(X_train_new, Y_train, validation_data=(X_Validation_new,Y_Validation), batch_size=12, epochs=maxepoch, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "root_path = ''\n",
    "# save the trained model\n",
    "model_yaml = transfered_model.to_yaml()\n",
    "with open(root_path+\"model_transfere-_learning_NAIP_100_samples_\"+timestr+\".yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "# save the weights\n",
    "transfered_model.save(root_path+\"model_transfere-_learning_NAIP_100_samples_\"+timestr+\".h5\")\n",
    "# save the intermdediate results and training statistics\n",
    "with open(root_path+\"history_transfere-_learning_NAIP_100_samples_\"+timestr+\".pickle\", 'wb') as file_pi:\n",
    "    pickle.dump(ltranfer_learning_results.history, file_pi, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will wait for the whole area data to do prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c574e7fc8f89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Save the predicted labels.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'ModelJun14/prediction_data.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpreds_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpreds_test_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpreds_test\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'ModelJun14/preds_test_total_attention2.npy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpreds_test_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the predicted labels.\n",
    "X_test = np.load(root_path+'ModelJun14/prediction_data.npy')\n",
    "preds_test = model.predict(X_test)\n",
    "preds_test_t = (preds_test > 0.5).astype(np.uint8)\n",
    "np.save(root_path+'ModelJun14/preds_test_total_attention2.npy',preds_test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progreess 10/12/2020\n",
    "\n",
    "Training from scratch   \n",
    "- Total params: 53,508,217  \n",
    "- Trainable params: 53,490,169  \n",
    "- Non-trainable params: 18,048  \n",
    "\n",
    "1. Traing the model from scratch without NAIP   \n",
    "        \n",
    "**Without NAIP**  \n",
    "The training takes 17 seconds for each epoch  \n",
    "- First training: the training stop at Epoch 105 (history_train_from_scratch_NoNAIP_20201012-111316)\n",
    "- We continue for 33 epochs more (history_train_from_scratch_NoNAIP_20201012-112416)\n",
    "    - Training set accu: 99.92%\n",
    "    - Validation set accu: 98.52%     \n",
    "\n",
    "**last line of log**   \n",
    "Epoch 33/400  \n",
    "50/50 [==============================] - 16s 313ms/step - loss: -0.9859 - dice_coef: 0.9859 - accuracy: 0.9992 - val_loss: -0.5574 - val_dice_coef: 0.5574 - val_accuracy: 0.9852\n",
    "\n",
    "1. Traing the model from scratch with NAIP   \n",
    "\n",
    "**With NAIP**   \n",
    "The training takes 17 seconds for each epoch \n",
    "- First training: the training stop at Epoch 141 (history_train_from_scratch_NAIP_20201012-121601)\n",
    "    - Training set accu: 99.91%\n",
    "    - Validation set accu: 98.87%   \n",
    "\n",
    "**last line of log**     \n",
    "Epoch 141/400   \n",
    "50/50 [==============================] - 16s 321ms/step - loss: -0.9849 - dice_coef: 0.9849 - accuracy: 0.9991 - val_loss: -0.6443 - val_dice_coef: 0.6443 - val_accuracy: 0.9887   \n",
    "  \n",
    "  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "  \n",
    "---- \n",
    "\n",
    "# Progress 28/09/2020    \n",
    "\n",
    "Established the baseline for **400 epochs**  \n",
    "                                                 \n",
    "Base line Model (Transfer learnign with NAIP data) can achieve validation acc. **92.89%**\n",
    "\n",
    "1. Added SpatialDropout2D layer    \n",
    "    **Result:** The dropout doesn't help in this case.     \n",
    "    with 0.3 drop rate and 400 epochs we can achieve validation acc. 92.63%   \n",
    "    with 0.5 drop rate and 400 epochs we can achieve validation acc. 92.32%  \n",
    "    with 0.7 drop rate and 400 epochs we can achieve validation acc. 92.06%   \n",
    "    \n",
    "    Compare to Dropout layer   \n",
    "    with 0.3 drop rate and 400 epochs we can achieve validation acc. 92.56%  \n",
    "    with 0.5 drop rate and 400 epochs we can achieve validation acc. 92.34%  \n",
    "    with 0.7 drop rate and 400 epochs we can achieve validation acc. 92.22%%  \n",
    "    \n",
    "2. Fine-tuning doesn't help much  \n",
    "    first pass learning rate 0.0001   \n",
    "    second pass learning rate 0.00001   \n",
    "    **Result:** we can achieve validation acc. 92.48%   \n",
    "   \n",
    "3. Woking on classifier after the convolutional network\n",
    "\n",
    "# Plan work\n",
    "1. Create colab notebook for RIF meeting \n",
    "2. Continue writing\n",
    "\n",
    "---\n",
    "\n",
    "# Progress 28/09/2020\n",
    "1. Added Dropout layer    \n",
    "    **Result:** The dropout doesn't help in this case.   \n",
    "    with 0.3 drop rate and 400 epochs we can achive 92.56%  \n",
    "    with 0.5 drop rate and 1000 epochs we can achive 94.1%  \n",
    "    with 0.7 drop rate and 400 epochs we can achive 92.2%  \n",
    "    \n",
    "2. Working on Fine-tuning process \n",
    "3. Working on Literature review\n",
    "\n",
    "\n",
    "# Plan work\n",
    "\n",
    "1. Finsihing the Literature review \n",
    "2. Try adding classifier after the convolutional network\n",
    "3. Begin the intro and abstract\n",
    "\n",
    "---\n",
    "\n",
    "# Progress 21/09/2020\n",
    "1. Genreating the result for transfer learnign without NAIP again \n",
    "2. Created the [plan until Mid Oct 2020](\"https://docs.google.com/document/d/1Kqz18zgB-DSkDarr-m8Y-__0ZCNzXAkTZw7Xg7kIy08/edit#\")\n",
    "    - The goal is to finish the first draft by Mid Oct. \n",
    "    \n",
    "# Plan work\n",
    "1. Start writing the paper\n",
    "2. Try training the model with more weight of stream class.\n",
    "3. Weekly plan until Mid October 2020[.]('https://analyticsindiamag.com/top-10-papers-on-transfer-learning-one-must-read-in-2020/')  \n",
    "\n",
    "--- \n",
    "\n",
    "# Progress 14/09/2020\n",
    "1. Read and summarize more  [transfer learning paper]('https://openreview.net/pdf?id=ryxyCeHtPB')  \n",
    "    - propose \"attentive feature distillation and selection (AFDS)\"   \n",
    "    - AFDS dynamically learns not only the features to transfer, but also the unimportant neurons to skip    \n",
    "    \n",
    "\n",
    "# Plan work\n",
    "1. Start writing the paper\n",
    "2. Try training the model with more weight of stream class.\n",
    "3. Weekly plan until Mid October 2020\n",
    "\n",
    "**Qual Exam beginning of next semester**\n",
    "\n",
    "---\n",
    "\n",
    "# Progress 07/09/2020  \n",
    "1. Presented the progress in CEGIS  \n",
    "  \n",
    "2. Generated total dataset for Covingtoin area (without NAIP imagery)\n",
    "    \n",
    "3. Run prediction of the Convington area with the dataset without NAIP and using the original model that is trained on Rowan creek area  \n",
    "\n",
    "\n",
    "# Plan work\n",
    "1. Try training the model with more weight of stream class.\n",
    "\n",
    "---\n",
    "\n",
    "# Progress 31/08/2020\n",
    "1. Corrected the data (removing None class (-9999) from test dataset)\n",
    "    - will generate the new test results  \n",
    "  \n",
    "  \n",
    "2. Preparing for CEGIS presentation\n",
    "    - Added prelim results  \n",
    "    - Will add the base scenario which is the U-net model predict the dataset without NAIP in Covinton river  \n",
    "      \n",
    "        \n",
    "    \n",
    "3. preparing the script for the presentation  \n",
    "    \n",
    "# Plan for this week\n",
    "1. Finish the presentation for CEGIS\n",
    "2. Read and summarize more paper\n",
    "3. Try training the model with more weight of stream class.\n",
    "\n",
    "----\n",
    "\n",
    "# Progress 24/08/2020\n",
    "\n",
    "**Comments:** Try to get the why and what it hold true and how to make or to apply to other places.  \n",
    "\n",
    "1. Generate the whole area and do testing\n",
    "    - Generated the dataset\n",
    "    - Evaluated the testing data and generated the prelim results\n",
    "**Problem:** the data has more than 2 classes as shown in evaluation.   \n",
    "      \n",
    "    \n",
    "2. Created the outline of the presentation for CEGIS \n",
    "    - Still need more details:   \n",
    "    https://docs.google.com/presentation/d/1PWrlgGEMCCJLXsAHeiTe40xdA22RtISE6gUpRbbplRs/edit?usp=sharing\n",
    "\n",
    "# Plan for this week\n",
    "1. Finish the presentation for CEGIS\n",
    "2. Read and summarize more paper\n",
    "3. Try training the model with more weight of stream class.\n",
    "4. Correct the data (remove the None class)\n",
    "\n",
    "---\n",
    "\n",
    "# Progress 17/08/2020\n",
    "1. Finished generating the new dataset\n",
    "    - Cleaned the NAIP data and all raw data of Covington River\n",
    "    - Included NAIP imagery into the dataset\n",
    "    - Edited the data preprocessing script to make it easier to add or remove data \n",
    "    - Added script documents and comments  \n",
    "      \n",
    "2. Generating the whole area dataset the included NAIP imagery\n",
    "    - Using High memory node on Keeling   \n",
    "    - **Problem:** The VPN disconnected after 2 hours in!!! T_T I have to start over.  \n",
    "  \n",
    "3. Trained the model with new dataset  \n",
    "    - The performance is significatly higher than the dataset without NAIP  \n",
    "  \n",
    "4. Read more papers and added summary of the read paper\n",
    "    -https://docs.google.com/document/d/1BApPn0aWTwstEpbnKC9g0p5KSOhi74_rF7nzRYM9CtE/edit  \n",
    "  \n",
    "# Plan for this week\n",
    "1. Generate the whole area and do testing\n",
    "2. Start preparing the presentation for CEGIS \n",
    "3. Read and summarize more papers\n",
    "    - Focus more on machine learning in hydro, remote sensing classification.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# Prgress 10/08/2020\n",
    "\n",
    "1. Successfully trained the model on my own PC.  \n",
    "    - Fixed cuCNN and CUDA version problems \n",
    "    - Trained with 4 trainable layers  \n",
    "      **Problem:** The model just disrtegards the stream class.  \n",
    "      **Root cause:** Unbalanced sample of stream and non-stream classes   \n",
    "\n",
    "2. In progress: Adding NAIP image to the dataset. \n",
    "    - Extracted the NAIP for Covinton and put it on Keeling \n",
    "    - modifying the preprocessing code\n",
    "    \n",
    "3. Outline the Introduction of the paper and reviewed some papers\n",
    "    - https://docs.google.com/document/d/1BApPn0aWTwstEpbnKC9g0p5KSOhi74_rF7nzRYM9CtE/edit\n",
    "    \n",
    "# Plan for this week\n",
    "1. Finished adding the NAIP and train the model again\n",
    "2. Start the first draft of the introduction \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
