{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE1f7hXLDQUM"
      },
      "source": [
        "# Model Training and Transfer Notebook\n",
        "\n",
        "## Description:\n",
        "This Colab notebook provides a comprehensive exploration of model training and transfer scenarios using various initialization strategies. The notebook showcases four distinct scenarios, each shedding light on the impact of different weight initialization techniques on model performance. Through a step-by-step guide, users gain insights into how models are trained, initialized, and transferred between different locations.\n",
        "\n",
        "## Scenarios:\n",
        "\n",
        "### 1. Random Initialization:\n",
        "In this scenario, the notebook demonstrates the training process of a model from scratch. The model is initialized with random weights, and the notebook elucidates how this initialization strategy affects the learning process and eventual convergence. Users will grasp the nuances of training a model with no prior knowledge.\n",
        "\n",
        "### 2. Pre-trained ImageNet Initialization:\n",
        "This section delves into the utilization of pre-trained weights from the ImageNet dataset. The notebook elucidates the advantages of leveraging pre-trained weights as initializations. Users witness the accelerated convergence and improved performance achieved when the model starts with learned features.\n",
        "\n",
        "### 3. Location Transfer with Random Initialization:\n",
        "The notebook examines the scenario of transferring a model trained at one location to another. It employs the model from Scenario 1 as the source and demonstrates how to adapt it to a new task or dataset. This illustrates the adaptability of models to novel contexts while using random initialization.\n",
        "\n",
        "### 4. Location Transfer with ImageNet-initialized Models:\n",
        "Expanding on the third scenario, this section focuses on transferring a model across locations while using the model from Scenario 2 as the source. Users witness how pre-trained weights enhance transfer learning, enabling the model to quickly adapt to new environments and tasks.\n",
        "\n",
        "Through clear explanations, code examples, and visualizations, this Colab notebook equips users with the knowledge and skills to effectively initialize, train, and transfer models. Whether experimenting with random weights or harnessing the power of pre-trained networks, users will gain a deeper understanding of the intricate dynamics that influence model behavior in various scenarios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcpqnEPlGJGx"
      },
      "source": [
        "## Initialized the environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "KYTF3VSvSjBs",
        "outputId": "88c0f304-5397-4d1c-a033-b2479d9d4eb5"
      },
      "outputs": [],
      "source": [
        "%pip install segmentation-models &> /dev/null\n",
        "%load_ext tensorboard\n",
        "\n",
        "# https://stackoverflow.com/questions/75433717/module-keras-utils-generic-utils-has-no-attribute-get-custom-objects-when-im\n",
        "# open the file keras.py, change all the 'init_keras_custom_objects' to 'init_tfkeras_custom_objects'.\n",
        "# the location of the keras.py is in the error message. In your case, it should be in /usr/local/lib/python3.8/dist-packages/efficientnet/\n",
        "!cp 'libskeras.py' '/usr/local/lib/python3.10/dist-packages/efficientnet/keras.py'\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import segmentation_models as sm\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.callbacks import (EarlyStopping, ModelCheckpoint,\n",
        "                                        ReduceLROnPlateau, TensorBoard)\n",
        "from tensorflow.keras.layers import Conv2D, Input\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "\n",
        "from unet_util import (UNET_224, Residual_CNN_block,\n",
        "                       attention_up_and_concatenate,\n",
        "                       attention_up_and_concatenate2, dice_coef,\n",
        "                       dice_coef_loss, evaluate_prediction_result, jacard_coef,\n",
        "                       multiplication, multiplication2)\n",
        "\n",
        "sm.set_framework('tf.keras')\n",
        "sm.framework()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqNonMpvS2Dj",
        "outputId": "4c9f06c2-2afa-45d4-9ce2-99542d467652"
      },
      "outputs": [],
      "source": [
        "name_id = '06302023' #You can change the id for each run so that all models and stats are saved separately.\n",
        "input_data = './samples/'\n",
        "prediction_path = './predicts_rerun_covington_'+name_id+'/'\n",
        "log_path = './logs_rerun_covington_'+name_id+'/'\n",
        "model_path = './models_rerun_covington_'+name_id+'/'\n",
        "save_model_path = './model_rerun_covington_'+name_id+'/'\n",
        "\n",
        "# Create the folder if it does not exist\n",
        "os.makedirs(input_data, exist_ok=True)\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "os.makedirs(prediction_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1t7lIENwhAM"
      },
      "source": [
        "## Select the models and choose the location\n",
        "\n",
        "The available backbones are listed here. You can copy and paste them in to the pbackends array. The available samples are listed here you can choose one location to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZURZXRpS3Fj"
      },
      "outputs": [],
      "source": [
        "# Avaiable backbones for Unet architechture\n",
        "# 'vgg16' 'vgg19' 'resnet18' 'resnet34' 'resnet50' 'resnet101' 'resnet152' 'inceptionv3'\n",
        "# 'inceptionresnetv2' 'densenet121' 'densenet169' 'densenet201' 'seresnet18' 'seresnet34'\n",
        "# 'seresnet50' 'seresnet101' 'seresnet152', and 'attentionUnet'\n",
        "backends = [\n",
        "    'resnet50', 'attentionUnet', 'densenet121','densenet169'\n",
        "]\n",
        "\n",
        "# Data location\n",
        "# 'covington' 'rowancreek'\n",
        "location = 'covington'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYFRrQDXxNBj"
      },
      "source": [
        "### Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "_OvWxpe4X4Q6",
        "outputId": "cc7295db-5690-489d-df0f-8b7e5faadceb"
      },
      "outputs": [],
      "source": [
        "X_train = np.load(input_data+location+'/train_data.npy').astype(np.float32)\n",
        "Y_train = np.load(input_data+location+'/train_label.npy').astype(np.float32)\n",
        "X_validation = np.load(input_data+location+'/vali_data.npy').astype(np.float32)\n",
        "Y_validation = np.load(input_data+location+'/vali_label.npy').astype(np.float32)\n",
        "# load the test data\n",
        "X_test = np.load(input_data+location+'/bottom_half_test_data.npy').astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXTGzi-3xZwV"
      },
      "source": [
        "## Scenario 1: Random Initialization Training\n",
        "\n",
        "the training process of a model with randomly initialized weights. This serves as a baseline for understanding how models evolve and adapt to a specific task when starting from scratch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgB9IGKsS_kt",
        "outputId": "800ff072-15b0-49e7-b0d9-a92ebdbb7050"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning flag\n",
        "# Set to False to random initialize the model\n",
        "finetune = False\n",
        "\n",
        "for backend in backends:\n",
        "\n",
        "  name = location+'-Unet-'+ backend + ('-tf' if(finetune) else '')\n",
        "\n",
        "  logdir = log_path + name\n",
        "  if(os.path.isdir(logdir)):\n",
        "    shutil.rmtree(logdir)\n",
        "  os.makedirs(logdir, exist_ok=True)\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "  print('model location: '+ model_path+name+'.h5')\n",
        "\n",
        "  # Create U-net model with the chosen backbone\n",
        "  if (backend==\"attentionUnet\"):\n",
        "    # Attention U-net model\n",
        "    learning_rate = 0.0000359\n",
        "    model = UNET_224()\n",
        "    model.compile(optimizer = Adam(learning_rate=learning_rate),\n",
        "                  loss = dice_coef_loss,\n",
        "                  metrics = [dice_coef,'accuracy'])\n",
        "  else:\n",
        "    if (not finetune):\n",
        "      # Unet without ImageNet backends\n",
        "      base_model = sm.Unet(backend, classes = 1, encoder_weights=None, input_shape=(None, None, 3))\n",
        "\n",
        "    else:\n",
        "      # Unet with ImageNet backends\n",
        "      base_model = sm.Unet(backend, classes = 1, encoder_weights = 'imagenet', encoder_freeze = finetune)\n",
        "\n",
        "    # The backbones are trained RGB so we need to add new input wiht 8 channels\n",
        "    # Conv2D will convert 8 channels input to 3 channels input for the pretrained backbones\n",
        "    inp = Input(shape=(None, None, 8))\n",
        "    l1 = Conv2D(3, (1, 1))(inp) # map N channels data to 3 channels\n",
        "    out = base_model(l1)\n",
        "    model = Model(inp, out, name = base_model.name)\n",
        "\n",
        "    # Compile the model with 'Adam' optimizer (0.001 is the default learning rate) and define the loss and metrics\n",
        "    model.compile(optimizer = Adam(),\n",
        "                  loss = dice_coef_loss,\n",
        "                  metrics=[dice_coef,'accuracy'])\n",
        "\n",
        "  # define hyperparameters and callback modules\n",
        "  patience = 10\n",
        "  maxepoch = 500\n",
        "  callbacks = [ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=patience, min_lr=1e-9, verbose=1, mode='min'),\n",
        "              EarlyStopping(monitor='val_loss', patience=patience, verbose=0),\n",
        "              ModelCheckpoint(model_path+name+'.h5', monitor='val_loss', save_best_only=True, verbose=0),\n",
        "              TensorBoard(log_dir=logdir)]\n",
        "\n",
        "  train_history = model.fit(x = X_train,y = Y_train,\n",
        "                            validation_data = (X_validation, Y_validation),\n",
        "                            batch_size = 16, epochs = maxepoch, verbose=0, callbacks = callbacks)\n",
        "\n",
        "  if(finetune and backend != \"attentionUnet\"):\n",
        "\n",
        "    # For fine-tuning we need to set the tranable flag to true for the whole model\n",
        "    model.trainable = True\n",
        "\n",
        "    # Recompile the model with the smaller learning rate at the optimizer (Adam(1e-5))\n",
        "    model.compile(optimizer = Adam(1e-5), loss = dice_coef_loss, metrics=[dice_coef,'accuracy'])\n",
        "\n",
        "    # train the model again\n",
        "    train_history_2 = model.fit(x = X_train, y = Y_train,\n",
        "                                validation_data=(X_validation, Y_validation),\n",
        "                                batch_size=16,epochs=maxepoch,\n",
        "                                initial_epoch = len(train_history.history['val_loss'])-1,\n",
        "                                verbose=0 ,callbacks=callbacks)\n",
        "\n",
        "    # Load the best model saved by the callback module\n",
        "  if(backend != \"attentionUnet\"):\n",
        "    best_model = load_model(model_path+name+'.h5',\n",
        "                            custom_objects={'dice_coef_loss':dice_coef_loss,\n",
        "                                            'dice_coef':dice_coef,})\n",
        "  else:\n",
        "    best_model = load_model(model_path+name+'.h5',\n",
        "                            custom_objects={'multiplication': multiplication,\n",
        "                                            'multiplication2': multiplication2,\n",
        "                                            'dice_coef_loss':dice_coef_loss,\n",
        "                                            'dice_coef':dice_coef,})\n",
        "\n",
        "  # predict the test data using the loaded model\n",
        "  test_predicted= best_model.predict(X_test)\n",
        "\n",
        "  # convert the prediction probability to true or false with threshold at 0.5\n",
        "  test_predicted_threshold = (test_predicted > 0.5).astype(np.uint8)\n",
        "\n",
        "  # save the prediction results\n",
        "  np.save(prediction_path+name+'_predict.npy',test_predicted_threshold)\n",
        "  print('Predtion results saved: ' + prediction_path+name+'_predict.npy')\n",
        "\n",
        "  pred_npy = prediction_path+name+'_predict.npy'\n",
        "  mask_npy = input_data+location+'/bottom_half_test_mask.npy'\n",
        "  label_npy = input_data+location+'/bottom_half_test_label.npy'\n",
        "  model = model_path + name + '.h5'\n",
        "  text_path = prediction_path+'prediction_results.txt'\n",
        "\n",
        "  evaluate_prediction_result(location, pred_npy, mask_npy, label_npy, model, text_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl0ihBYiyUyH"
      },
      "source": [
        "## Scenario 2: Pre-trained ImageNet Initialization Training\n",
        "\n",
        "The second scenario showcases the training process of a model initialized with pre-trained weights from the ImageNet dataset. This highlights the benefits of leveraging transfer learning and how pre-existing knowledge can expedite convergence and enhance overall performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECCT27GZ1w08",
        "outputId": "e31e4738-0349-468e-a971-27a6c3b30882"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning flag\n",
        "# the flag is set to True to allow the model to be initialized by ImageNet pre-trained weights\n",
        "finetune = True\n",
        "\n",
        "for backend in backends:\n",
        "\n",
        "  name = location+'-Unet-'+ backend + ('-tf' if(finetune) else '')\n",
        "\n",
        "  logdir = log_path + name\n",
        "  if(os.path.isdir(logdir)):\n",
        "    shutil.rmtree(logdir)\n",
        "  os.makedirs(logdir, exist_ok=True)\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "  print('model location: '+ model_path+name+'.h5')\n",
        "\n",
        "  # Create U-net model with the chosen backbone\n",
        "\n",
        "  if (backend==\"attentionUnet\"):\n",
        "    # Attention U-net model\n",
        "    learning_rate = 0.0000359\n",
        "    model = UNET_224()\n",
        "    model.compile(optimizer = Adam(learning_rate=learning_rate),\n",
        "                  loss = dice_coef_loss,\n",
        "                  metrics = [dice_coef,'accuracy'])\n",
        "  else:\n",
        "    if (not finetune):\n",
        "      # Unet with ImageNet backends\n",
        "      base_model = sm.Unet(backend, classes = 1, encoder_weights=None, input_shape=(None, None, 3))\n",
        "      # name = name + '-addedlayer'\n",
        "    else:\n",
        "      # Unet with ImageNet backends\n",
        "      base_model = sm.Unet(backend, classes = 1, encoder_weights = 'imagenet', encoder_freeze = finetune)\n",
        "\n",
        "    # The backbones are trained RGB so we need to add new input wiht 8 channels\n",
        "    # Conv2D will convert 8 channels input to 3 channels input for the pretrained backbones\n",
        "    inp = Input(shape=(None, None, 8))\n",
        "    l1 = Conv2D(3, (1, 1))(inp) # map N channels data to 3 channels\n",
        "    out = base_model(l1)\n",
        "    model = Model(inp, out, name = base_model.name)\n",
        "\n",
        "    # Compile the model with 'Adam' optimizer (0.001 is the default learning rate) and define the loss and metrics\n",
        "    model.compile(optimizer = Adam(),\n",
        "                  loss = dice_coef_loss,\n",
        "                  metrics=[dice_coef,'accuracy'])\n",
        "\n",
        "  # define hyperparameters and callback modules\n",
        "  patience = 10\n",
        "  maxepoch = 500\n",
        "  callbacks = [ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=patience, min_lr=1e-9, verbose=1, mode='min'),\n",
        "              EarlyStopping(monitor='val_loss', patience=patience, verbose=0),\n",
        "              ModelCheckpoint(model_path+name+'.h5', monitor='val_loss', save_best_only=True, verbose=0),\n",
        "              TensorBoard(log_dir=logdir)]\n",
        "\n",
        "  train_history = model.fit(x = X_train,y = Y_train,\n",
        "                            validation_data = (X_validation, Y_validation),\n",
        "                            batch_size = 16, epochs = maxepoch, verbose=0, callbacks = callbacks)\n",
        "\n",
        "  if(finetune and backend != \"attentionUnet\"):\n",
        "\n",
        "    # For fine-tuning we need to set the tranable flag to true for the whole model\n",
        "    model.trainable = True\n",
        "\n",
        "    # Recompile the model with the smaller learning rate at the optimizer (Adam(1e-5))\n",
        "    model.compile(optimizer = Adam(1e-5), loss = dice_coef_loss, metrics=[dice_coef,'accuracy'])\n",
        "\n",
        "    # train the model again\n",
        "    train_history_2 = model.fit(x = X_train, y = Y_train,\n",
        "                                validation_data=(X_validation, Y_validation),\n",
        "                                batch_size=16,epochs=maxepoch,\n",
        "                                initial_epoch = len(train_history.history['val_loss'])-1,\n",
        "                                verbose=0 ,callbacks=callbacks)\n",
        "\n",
        "    # Load the best model saved by the callback module\n",
        "  from keras.models import load_model\n",
        "  if(backend != \"attentionUnet\"):\n",
        "    best_model = load_model(model_path+name+'.h5',\n",
        "                            custom_objects={'dice_coef_loss':dice_coef_loss,\n",
        "                                            'dice_coef':dice_coef,})\n",
        "  else:\n",
        "    best_model = load_model(model_path+name+'.h5',\n",
        "                            custom_objects={'multiplication': multiplication,\n",
        "                                            'multiplication2': multiplication2,\n",
        "                                            'dice_coef_loss':dice_coef_loss,\n",
        "                                            'dice_coef':dice_coef,})\n",
        "\n",
        "  # predict the test data using the loaded model\n",
        "  test_predicted= best_model.predict(X_test)\n",
        "\n",
        "  # convert the prediction probability to true or false with threshold at 0.5\n",
        "  test_predicted_threshold = (test_predicted > 0.5).astype(np.uint8)\n",
        "\n",
        "  # save the prediction results\n",
        "  np.save(prediction_path+name+'_predict.npy',test_predicted_threshold)\n",
        "  print('Predtion results saved: ' + prediction_path+name+'_predict.npy')\n",
        "\n",
        "  pred_npy = prediction_path+name+'_predict.npy'\n",
        "  mask_npy = input_data+location+'/bottom_half_test_mask.npy'\n",
        "  label_npy = input_data+location+'/bottom_half_test_label.npy'\n",
        "  model = model_path + name + '.h5'\n",
        "  text_path = prediction_path+'prediction_results.txt'\n",
        "\n",
        "  evaluate_prediction_result(location, pred_npy, mask_npy, label_npy, model, text_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJAEqs3hyyva"
      },
      "source": [
        "## Scenario 3: Location Transfer with Random Initialization\n",
        "\n",
        " It employs the model from Scenario 1 as the source and demonstrates how to adapt it to a new task or dataset. This illustrates the adaptability of models to novel contexts while using random initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "yZ4nCHVvCrSs",
        "outputId": "49c31e2a-6772-408b-9fc2-87a4eab2c706"
      },
      "outputs": [],
      "source": [
        "# Source Location\n",
        "# 'covington' 'rowancreek'\n",
        "source_location = 'rowancreek'\n",
        "\n",
        "# Target Location\n",
        "# 'covington' 'rowancreek'\n",
        "target_location = 'covington'\n",
        "\n",
        "# Fine-tuning flag\n",
        "# True/False\n",
        "finetune = True\n",
        "\n",
        "# Test no ImageNet intialized weights\n",
        "noImagenet = False\n",
        "\n",
        "for backend in backends:\n",
        "\n",
        "  # Construct the model save file name\n",
        "  source_name = source_location+'-Unet-'+backend + ('-tf' if(finetune) else '')\n",
        "\n",
        "\n",
        "  # Construct the model save file name\n",
        "  tareget_name = source_location+'-to-'+target_location+'-Unet-'+backend + ('-noImgN' if(noImagenet) else '-ImgN')\n",
        "\n",
        "  logdir = log_path + tareget_name\n",
        "  if(os.path.isdir(logdir)):\n",
        "    shutil.rmtree(logdir)\n",
        "  os.makedirs(logdir, exist_ok=True)\n",
        "  # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "  print('source model location: '+ model_path+source_name+'.h5')\n",
        "  print('target model location: '+ model_path+tareget_name+'.h5')\n",
        "\n",
        "  # define hyperparameters and callback modules\n",
        "  patience = 10\n",
        "  maxepoch = 500\n",
        "  callbacks = [ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=patience, min_lr=1e-9, verbose=1, mode='min'),\n",
        "              EarlyStopping(monitor='val_loss', patience=patience, verbose=0),\n",
        "              ModelCheckpoint(save_model_path+tareget_name+'.h5', monitor='val_loss', save_best_only=True, verbose=0),\n",
        "              TensorBoard(log_dir=logdir)]\n",
        "\n",
        "  # Load the best model saved by the callback module\n",
        "  if(backend != \"attentionUnet\"):\n",
        "    model = load_model(model_path+source_name+'.h5',\n",
        "                          custom_objects={'dice_coef_loss':dice_coef_loss,\n",
        "                                          'dice_coef':dice_coef,})\n",
        "  else:\n",
        "    model = load_model(model_path+source_name+'.h5',\n",
        "                          custom_objects={'multiplication': multiplication,\n",
        "                                          'multiplication2': multiplication2,\n",
        "                                          'dice_coef_loss':dice_coef_loss,\n",
        "                                          'dice_coef':dice_coef,})\n",
        "\n",
        "\n",
        "  train_history = model.fit(x = X_train,y = Y_train,\n",
        "                            validation_data = (X_validation, Y_validation),\n",
        "                            batch_size = 16, epochs = maxepoch, verbose=0, callbacks = callbacks)\n",
        "\n",
        "  if(finetune or noImagenet):\n",
        "\n",
        "    # For fine-tuning we need to set the tranable flag to true for the whole model\n",
        "    model.trainable = True\n",
        "\n",
        "    # Recompile the model with the smaller learning rate at the optimizer (Adam(1e-5))\n",
        "    model.compile(optimizer = Adam(1e-5), loss = dice_coef_loss, metrics=[dice_coef,'accuracy'])\n",
        "\n",
        "    # train the model again\n",
        "    train_history_2 = model.fit(x = X_train, y = Y_train,\n",
        "                                validation_data=(X_validation, Y_validation),\n",
        "                                batch_size=16,epochs=maxepoch,\n",
        "                                initial_epoch = len(train_history.history['val_loss'])-1,\n",
        "                                verbose=0 ,callbacks=callbacks)\n",
        "\n",
        "  # Load the best model saved by the callback module\n",
        "  if(backend != \"attentionUnet\"):\n",
        "    best_model = load_model(save_model_path+tareget_name+'.h5',\n",
        "                          custom_objects={'dice_coef_loss':dice_coef_loss,\n",
        "                                          'dice_coef':dice_coef,})\n",
        "  else:\n",
        "    best_model = load_model(save_model_path+tareget_name+'.h5',\n",
        "                          custom_objects={'multiplication': multiplication,\n",
        "                                          'multiplication2': multiplication2,\n",
        "                                          'dice_coef_loss':dice_coef_loss,\n",
        "                                          'dice_coef':dice_coef,})\n",
        "\n",
        "  # predict the test data using the loaded model\n",
        "  test_predicted= best_model.predict(X_test)\n",
        "\n",
        "  # convert the prediction probability to true or false with threshold at 0.5\n",
        "  test_predicted_threshold = (test_predicted > 0.5).astype(np.uint8)\n",
        "\n",
        "  # save the prediction results\n",
        "  np.save(prediction_path+tareget_name+'_predict.npy',test_predicted_threshold)\n",
        "  print('Predtion results saved: ' + prediction_path+tareget_name+'_predict.npy')\n",
        "\n",
        "  # pred_npy = prediction_path+tareget_name+'_predict.npy'\n",
        "  # mask_npy = input_data+target_location+'/bottom_half_test_mask.npy'\n",
        "  # label_npy = input_data+target_location+'/bottom_half_test_label.npy'\n",
        "  # model = save_model_path + tareget_name + '.h5'\n",
        "  # text_path = prediction_path+'prediction_results.txt'\n",
        "\n",
        "  # evaluate_prediction_result(target_location, pred_npy, mask_npy, label_npy, model, text_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwkyWbpEzwYm"
      },
      "source": [
        "## Scenario 4: Location Transfer with ImageNet-initialized Models\n",
        "\n",
        "Expanding on the third scenario, this section focuses on transferring a model across locations while using the model from Scenario 2 as the source. Users witness how pre-trained weights enhance transfer learning, enabling the model to quickly adapt to new environments and tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaWIla1yYEKr",
        "outputId": "112e3a28-88e9-47c7-9469-07194cd48804"
      },
      "outputs": [],
      "source": [
        "# Source Location\n",
        "# 'covington' 'rowancreek'\n",
        "source_location = 'rowancreek'\n",
        "\n",
        "# Target Location\n",
        "# 'covington' 'rowancreek'\n",
        "target_location = 'covington'\n",
        "\n",
        "# Fine-tuning flag\n",
        "# True/False\n",
        "finetune = True\n",
        "\n",
        "# Test no ImageNet intialized weights\n",
        "noImagenet = True\n",
        "\n",
        "\n",
        "for backend in backends:\n",
        "\n",
        "  # Construct the model save file name\n",
        "  source_name = source_location+'-Unet-'+backend + ('-tf' if(finetune) else '')\n",
        "\n",
        "\n",
        "  # Construct the model save file name\n",
        "  tareget_name = source_location+'-to-'+target_location+'-Unet-'+backend + ('-noImgN' if(noImagenet) else '-ImgN')\n",
        "\n",
        "  logdir = log_path + tareget_name\n",
        "  if(os.path.isdir(logdir)):\n",
        "    shutil.rmtree(logdir)\n",
        "  os.makedirs(logdir, exist_ok=True)\n",
        "  # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "  print('source model location: '+ model_path+source_name+'.h5')\n",
        "  print('target model location: '+ model_path+tareget_name+'.h5')\n",
        "\n",
        "  # define hyperparameters and callback modules\n",
        "  patience = 10\n",
        "  maxepoch = 500\n",
        "  callbacks = [ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=patience, min_lr=1e-9, verbose=1, mode='min'),\n",
        "              EarlyStopping(monitor='val_loss', patience=patience, verbose=0),\n",
        "              ModelCheckpoint(save_model_path+tareget_name+'.h5', monitor='val_loss', save_best_only=True, verbose=0),\n",
        "              TensorBoard(log_dir=logdir)]\n",
        "\n",
        "  # Load the best model saved by the callback module\n",
        "  if(backend != \"attentionUnet\"):\n",
        "    model = load_model(model_path+source_name+'.h5',\n",
        "                          custom_objects={'dice_coef_loss':dice_coef_loss,\n",
        "                                          'dice_coef':dice_coef,})\n",
        "  else:\n",
        "    model = load_model(model_path+source_name+'.h5',\n",
        "                          custom_objects={'multiplication': multiplication,\n",
        "                                          'multiplication2': multiplication2,\n",
        "                                          'dice_coef_loss':dice_coef_loss,\n",
        "                                          'dice_coef':dice_coef,})\n",
        "\n",
        "\n",
        "  train_history = model.fit(x = X_train,y = Y_train,\n",
        "                            validation_data = (X_validation, Y_validation),\n",
        "                            batch_size = 16, epochs = maxepoch, verbose=0, callbacks = callbacks)\n",
        "\n",
        "  if(finetune or noImagenet):\n",
        "\n",
        "    # For fine-tuning we need to set the tranable flag to true for the whole model\n",
        "    model.trainable = True\n",
        "\n",
        "    # Recompile the model with the smaller learning rate at the optimizer (Adam(1e-5))\n",
        "    model.compile(optimizer = Adam(1e-5), loss = dice_coef_loss, metrics=[dice_coef,'accuracy'])\n",
        "\n",
        "    # train the model again\n",
        "    train_history_2 = model.fit(x = X_train, y = Y_train,\n",
        "                                validation_data=(X_validation, Y_validation),\n",
        "                                batch_size=16,epochs=maxepoch,\n",
        "                                initial_epoch = len(train_history.history['val_loss'])-1,\n",
        "                                verbose=0 ,callbacks=callbacks)\n",
        "\n",
        "  # Load the best model saved by the callback module\n",
        "  if(backend != \"attentionUnet\"):\n",
        "    best_model = load_model(save_model_path+tareget_name+'.h5',\n",
        "                          custom_objects={'dice_coef_loss':dice_coef_loss,\n",
        "                                          'dice_coef':dice_coef,})\n",
        "  else:\n",
        "    best_model = load_model(save_model_path+tareget_name+'.h5',\n",
        "                          custom_objects={'multiplication': multiplication,\n",
        "                                          'multiplication2': multiplication2,\n",
        "                                          'dice_coef_loss':dice_coef_loss,\n",
        "                                          'dice_coef':dice_coef,})\n",
        "\n",
        "  # predict the test data using the loaded model\n",
        "  test_predicted= best_model.predict(X_test)\n",
        "\n",
        "  # convert the prediction probability to true or false with threshold at 0.5\n",
        "  test_predicted_threshold = (test_predicted > 0.5).astype(np.uint8)\n",
        "\n",
        "  # save the prediction results\n",
        "  np.save(prediction_path+tareget_name+'_predict.npy',test_predicted_threshold)\n",
        "  print('Predtion results saved: ' + prediction_path+tareget_name+'_predict.npy')\n",
        "\n",
        "  pred_npy = prediction_path+tareget_name+'_predict.npy'\n",
        "  mask_npy = input_data+target_location+'/bottom_half_test_mask.npy'\n",
        "  label_npy = input_data+target_location+'/bottom_half_test_label.npy'\n",
        "  model = save_model_path + tareget_name + '.h5'\n",
        "  text_path = prediction_path+'prediction_results.txt'\n",
        "\n",
        "  evaluate_prediction_result(target_location, pred_npy, mask_npy, label_npy, model, text_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 ('tensorflow-gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "95875376d21992e2bb2dfb532dd42fa3f9007a202209eebfcd016b20cc8adbec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
