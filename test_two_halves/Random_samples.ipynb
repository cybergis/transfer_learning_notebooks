{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_path = '../Covington_data/without_NAIP/nodata_as_0/'\n",
    "name = 'No_NAIP_'\n",
    "# read in training and validation data\n",
    "X_train = np.load(data_path+'train_data.npy')\n",
    "Y_train = np.load(data_path+'train_label.npy')\n",
    "X_Validation = np.load(data_path+'vali_data.npy')\n",
    "Y_Validation = np.load(data_path+'vali_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import math    \n",
    "import os\n",
    "\n",
    "def generate_sample(size, save_path): \n",
    "    sample_size =size #10,20,30,40,...,100 \n",
    "\n",
    "    training_sample_ratio = 0.8\n",
    "    training =  math.ceil(sample_size * training_sample_ratio)\n",
    "    validation =  math.ceil(sample_size *(1-training_sample_ratio))\n",
    "\n",
    "    training_idx = random.sample(range(0,600),k=training)\n",
    "    validation_idx = random.sample(range(0,600),k=validation)\n",
    "\n",
    "    print(len(training_idx))\n",
    "    print(len(validation_idx))\n",
    "\n",
    "    train_data = [X_train[training_idx[0]]]\n",
    "    train_label = [Y_train[training_idx[0]]]\n",
    "    for train_index in range(1,len(training_idx)):\n",
    "        #print(train_index)\n",
    "        train_data = np.concatenate((train_data,[X_train[training_idx[train_index]]]))\n",
    "        train_label = np.concatenate((train_label,[Y_train[training_idx[train_index]]]))\n",
    "\n",
    "    valid_data = [X_Validation[validation_idx[0]]]\n",
    "    valid_label = [Y_Validation[validation_idx[0]]]\n",
    "    for val_index in range(1,len(validation_idx)):\n",
    "        #print(val_index)\n",
    "        valid_data = np.concatenate((valid_data,[X_Validation[validation_idx[val_index]]]))\n",
    "        valid_label = np.concatenate((valid_label,[Y_Validation[validation_idx[val_index]]]))\n",
    "\n",
    "\n",
    "#     print(train_data.shape)\n",
    "#     print(train_label.shape)\n",
    "#     print(valid_data.shape)\n",
    "#     print(valid_label.shape)\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    np.save(save_path+'train_data.npy',train_data)\n",
    "    np.save(save_path+'train_label.npy',train_label)\n",
    "    np.save(save_path+'vali_data.npy',valid_data)\n",
    "    np.save(save_path+'vali_label.npy',valid_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n",
      "40\n",
      "160\n",
      "40\n",
      "160\n",
      "40\n",
      "160\n",
      "40\n",
      "160\n",
      "40\n",
      "160\n",
      "40\n",
      "160\n",
      "40\n",
      "160\n",
      "40\n",
      "160\n",
      "40\n",
      "160\n",
      "40\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n",
      "320\n",
      "80\n",
      "320\n",
      "80\n",
      "320\n",
      "80\n",
      "320\n",
      "80\n",
      "320\n",
      "80\n",
      "320\n",
      "80\n",
      "320\n",
      "80\n",
      "320\n",
      "80\n",
      "320\n",
      "80\n",
      "320\n",
      "80\n",
      "400\n",
      "100\n",
      "400\n",
      "100\n",
      "400\n",
      "100\n",
      "400\n",
      "100\n",
      "400\n",
      "100\n",
      "400\n",
      "100\n",
      "400\n",
      "100\n",
      "400\n",
      "100\n",
      "400\n",
      "100\n",
      "400\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "for size in range(200,600,100):\n",
    "    for i in range(1,11):\n",
    "#         print(size, i)\n",
    "        save_path = \"./vary_small_samples/samples/\"+str(size)+\"/\"+str(i)+\"/\"\n",
    "        generate_sample(size,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4602aee97cc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;31m# # ##############################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# # 100 samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\random.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, population, k)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m21\u001b[0m        \u001b[1;31m# size of a small set minus size of an empty list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "# The dataset has 9 channels:\n",
    "# 0. Curvature\n",
    "# 1. Slope\n",
    "# 2. Openness\n",
    "# 3. DEM\n",
    "# 4. TPI 21\n",
    "# 5. Reflectance (LiDAR intensity)\n",
    "# 6. Geomorphon\n",
    "# 7. TPI 9\n",
    "# 8. TPI 3\n",
    "import random \n",
    "\n",
    "sample_size= 10 #10,20,30,40,...,100 \n",
    "\n",
    "training_sample_ratio = 0.8\n",
    "training = sample_size * training_sample_ratio\n",
    "validation = sample_size * (1-training_sample_ratio)\n",
    "\n",
    "\n",
    "print(random.sample(range(1,300),training))\n",
    "# # ##############################################\n",
    "# # 100 samples \n",
    "# X_train_new = X_train[0:80,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:80,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:20,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:20,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"100_samples\"\n",
    "# # ##############################################\n",
    "\n",
    "# # ##############################################\n",
    "# # 90 samples \n",
    "# X_train_new = X_train[0:72,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:72,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:18,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:18,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"90_samples_\"\n",
    "# # ##############################################\n",
    "\n",
    "# # ##############################################\n",
    "# # 80 samples \n",
    "# X_train_new = X_train[0:64,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:64,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:16,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:16,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"80_samples_\"\n",
    "# # ##############################################\n",
    "\n",
    "# ##############################################\n",
    "# 70 samples \n",
    "X_train_new = X_train[0:56,:,:,:]\n",
    "print(X_train_new.shape)\n",
    "Y_train = np.load(data_path+'train_label.npy')[0:56,:,:,:]\n",
    "print(Y_train.shape)\n",
    "\n",
    "X_Validation_new = X_Validation[0:14,:,:,:]\n",
    "print(X_Validation_new.shape)\n",
    "Y_Validation = np.load(data_path+'vali_label.npy')[0:14,:,:,:]\n",
    "print(Y_Validation.shape)\n",
    "name += \"70_samples_\"\n",
    "# ##############################################\n",
    "\n",
    "# # ##############################################\n",
    "# # 60 samples \n",
    "# X_train_new = X_train[0:48,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:48,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:12,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:12,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"60_samples_\"\n",
    "# # ##############################################\n",
    "\n",
    "# # ##############################################\n",
    "# # 50 samples \n",
    "# X_train_new = X_train[0:40,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:40,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:10,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:10,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"50_samples_\"\n",
    "# # ##############################################\n",
    "\n",
    "# # ##############################################\n",
    "# # 40 samples \n",
    "# X_train_new = X_train[0:32,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:32,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:8,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:8,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"40_samples_\"\n",
    "# # ##############################################\n",
    "\n",
    "# # ##############################################\n",
    "# # 30 samples \n",
    "# X_train_new = X_train[0:24,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:24,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:6,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:6,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"30_samples_\"\n",
    "# # ##############################################\n",
    "\n",
    "# ##############################################\n",
    "# # 20 samples \n",
    "# X_train_new = X_train[0:16,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:16,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:4,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:4,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"20_samples_\"\n",
    "# ##############################################\n",
    "\n",
    "\n",
    "# ##############################################\n",
    "# # 10 samples \n",
    "# X_train_new = X_train[0:8,:,:,:]\n",
    "# print(X_train_new.shape)\n",
    "# Y_train = np.load(data_path+'train_label.npy')[0:8,:,:,:]\n",
    "# print(Y_train.shape)\n",
    "\n",
    "# X_Validation_new = X_Validation[0:2,:,:,:]\n",
    "# print(X_Validation_new.shape)\n",
    "# Y_Validation = np.load(data_path+'vali_label.npy')[0:2,:,:,:]\n",
    "# print(Y_Validation.shape)\n",
    "# name += \"10_samples_\"\n",
    "# ##############################################\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
