{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "model_train_from_scratch_No_NAIP_200_samples_r1_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_200_samples_r2_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_200_samples_r3_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_200_samples_r4_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_200_samples_r5_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_200_samples_r6_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_200_samples_r7_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_200_samples_r8_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_200_samples_r9_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_200_samples_r10_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_300_samples_r1_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_300_samples_r2_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_300_samples_r3_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_300_samples_r4_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_300_samples_r5_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_300_samples_r6_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_300_samples_r7_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_300_samples_r8_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_300_samples_r9_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_300_samples_r10_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_400_samples_r1_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_400_samples_r2_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_400_samples_r3_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_400_samples_r4_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_400_samples_r5_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_400_samples_r6_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_400_samples_r7_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_400_samples_r8_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_400_samples_r9_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_400_samples_r10_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_500_samples_r1_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_500_samples_r2_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_500_samples_r3_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_500_samples_r4_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_500_samples_r5_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_500_samples_r6_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_500_samples_r7_\n",
      "exists\n",
      "model_train_from_scratch_No_NAIP_500_samples_r8_\n",
      "(400, 224, 224, 8)\n",
      "(400, 224, 224, 1)\n",
      "(100, 224, 224, 8)\n",
      "(100, 224, 224, 1)\n",
      "model_train_from_scratch_No_NAIP_500_samples_r8_\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node conv2d_1/convolution (defined at C:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_33646]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c3f5c9895494>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0mno_transfer_learning_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_Validation_new\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_Validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node conv2d_1/convolution (defined at C:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_33646]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "# Load all the dependencies\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from numpy import genfromtxt\n",
    "from tensorflow import random\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Layer, UpSampling2D, GlobalAveragePooling2D, Multiply, Dense, Reshape, Permute, multiply, dot, add, Input\n",
    "from keras.layers.core import Dropout, Lambda, SpatialDropout2D, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model, load_model, model_from_yaml, Sequential\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1337) # for reproducibility\n",
    "random.set_seed(1337)\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "# Use dice coefficient function as the loss function \n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
    "\n",
    "# Jacard coefficient\n",
    "def jacard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
    "\n",
    "# calculate loss value\n",
    "def jacard_coef_loss(y_true, y_pred):\n",
    "    return -jacard_coef(y_true, y_pred)\n",
    "\n",
    "# calculate loss value\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "def Residual_CNN_block(x, size, dropout=0.0, batch_norm=True):\n",
    "    if K.image_data_format() == 'th':\n",
    "        axis = 1\n",
    "    else:\n",
    "        axis = 3\n",
    "    conv = Conv2D(size, (3, 3), padding='same')(x)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Conv2D(size, (3, 3), padding='same')(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Conv2D(size, (3, 3), padding='same')(conv)\n",
    "    if batch_norm is True:\n",
    "        conv = BatchNormalization(axis=axis)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    return conv\n",
    "\n",
    "class multiplication(Layer):\n",
    "    def __init__(self,inter_channel = None,**kwargs):\n",
    "        super(multiplication, self).__init__(**kwargs)\n",
    "        self.inter_channel = inter_channel\n",
    "    def build(self,input_shape=None):\n",
    "        self.k = self.add_weight(name='k',shape=(1,),initializer='zeros',dtype='float32',trainable=True)\n",
    "    def get_config(self):\n",
    "        base_config = super(multiplication, self).get_config()\n",
    "        config = {'inter_channel':self.inter_channel}\n",
    "        return dict(list(base_config.items()) + list(config.items()))  \n",
    "    def call(self,inputs):\n",
    "        g,x,x_query,phi_g,x_value = inputs[0],inputs[1],inputs[2],inputs[3],inputs[4]\n",
    "        h,w,c = int(x.shape[1]),int(x.shape[2]),int(x.shape[3])\n",
    "        x_query = K.reshape(x_query, shape=(-1,h*w, self.inter_channel//4))\n",
    "        phi_g = K.reshape(phi_g,shape=(-1,h*w,self.inter_channel//4))\n",
    "        x_value = K.reshape(x_value,shape=(-1,h*w,c))\n",
    "        scale = dot([K.permute_dimensions(phi_g,(0,2,1)), x_query], axes=(1, 2))\n",
    "        soft_scale = Activation('softmax')(scale)\n",
    "        scaled_value = dot([K.permute_dimensions(soft_scale,(0,2,1)),K.permute_dimensions(x_value,(0,2,1))],axes=(1, 2))\n",
    "        scaled_value = K.reshape(scaled_value, shape=(-1,h,w,c))        \n",
    "        customize_multi = self.k * scaled_value\n",
    "        layero = add([customize_multi,x])\n",
    "        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n",
    "        concate = my_concat([layero,g])\n",
    "        return concate \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        ll = list(input_shape)[1]\n",
    "        return (None,ll[1],ll[1],ll[3]*3)\n",
    "    def get_custom_objects():\n",
    "        return {'multiplication': multiplication}\n",
    "\n",
    "def attention_up_and_concatenate(inputs):\n",
    "    g,x = inputs[0],inputs[1]\n",
    "    inter_channel = g.get_shape().as_list()[3]\n",
    "    g = Conv2DTranspose(inter_channel, (2,2), strides=[2, 2],padding='same')(g)\n",
    "    x_query = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(x)\n",
    "    phi_g = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(g)\n",
    "    x_value = Conv2D(inter_channel//2, [1, 1], strides=[1, 1], data_format='channels_last')(x)\n",
    "    inputs = [g,x,x_query,phi_g,x_value]\n",
    "    concate = multiplication(inter_channel)(inputs)\n",
    "    return concate\n",
    "\n",
    "class multiplication2(Layer):\n",
    "    def __init__(self,inter_channel = None,**kwargs):\n",
    "        super(multiplication2, self).__init__(**kwargs)\n",
    "        self.inter_channel = inter_channel\n",
    "    def build(self,input_shape=None):\n",
    "        self.k = self.add_weight(name='k',shape=(1,),initializer='zeros',dtype='float32',trainable=True)\n",
    "    def get_config(self):\n",
    "        base_config = super(multiplication2, self).get_config()\n",
    "        config = {'inter_channel':self.inter_channel}\n",
    "        return dict(list(base_config.items()) + list(config.items()))  \n",
    "    def call(self,inputs):\n",
    "        g,x,rate = inputs[0],inputs[1],inputs[2]\n",
    "        scaled_value = multiply([x, rate])\n",
    "        att_x =  self.k * scaled_value\n",
    "        att_x = add([att_x,x])\n",
    "        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n",
    "        concate = my_concat([att_x, g])\n",
    "        return concate \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        ll = list(input_shape)[1]\n",
    "        return (None,ll[1],ll[1],ll[3]*2)\n",
    "    def get_custom_objects():\n",
    "        return {'multiplication2': multiplication2}\n",
    "\n",
    "def attention_up_and_concatenate2(inputs):\n",
    "    g, x = inputs[0],inputs[1]\n",
    "    inter_channel = g.get_shape().as_list()[3]\n",
    "    g = Conv2DTranspose(inter_channel//2, (3,3), strides=[2, 2],padding='same')(g)\n",
    "    g = Conv2D(inter_channel//2, [1, 1], strides=[1, 1], data_format='channels_last')(g)\n",
    "    theta_x = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(x)\n",
    "    phi_g = Conv2D(inter_channel//4, [1, 1], strides=[1, 1], data_format='channels_last')(g)\n",
    "    f = Activation('relu')(add([theta_x, phi_g]))\n",
    "    psi_f = Conv2D(1, [1, 1], strides=[1, 1], data_format='channels_last')(f)\n",
    "    rate = Activation('sigmoid')(psi_f)\n",
    "    concate =  multiplication2()([g,x,rate])\n",
    "    return concate\n",
    "\n",
    "# The structure of the constructed U-net model\n",
    "def UNET_224(weights=None):\n",
    "    if K.image_data_format() == 'th':\n",
    "        inputs = Input((INPUT_CHANNELS, IMG_WIDTH, IMG_WIDTH))\n",
    "        axis = 1\n",
    "    else:\n",
    "        inputs = Input((IMG_WIDTH, IMG_WIDTH, INPUT_CHANNELS))\n",
    "        axis = 3\n",
    "    filters = 32\n",
    "    last_dropout = 0.2\n",
    "# convolutiona and pooling level 1\n",
    "    conv_224 = Residual_CNN_block(inputs,filters)\n",
    "    pool_112 = MaxPooling2D(pool_size=(2, 2))(conv_224)\n",
    "# convolutiona and pooling level 2\n",
    "    conv_112 = Residual_CNN_block(pool_112,2*filters)\n",
    "    pool_56 = MaxPooling2D(pool_size=(2, 2))(conv_112)\n",
    "# convolutiona and pooling level 3\n",
    "    conv_56 = Residual_CNN_block(pool_56,4*filters)\n",
    "    pool_28 = MaxPooling2D(pool_size=(2, 2))(conv_56)\n",
    "# convolutiona and pooling level 4\n",
    "    conv_28 = Residual_CNN_block(pool_28,8*filters)\n",
    "    pool_14 = MaxPooling2D(pool_size=(2, 2))(conv_28)\n",
    "# convolutiona and pooling level 5\n",
    "    conv_14 = Residual_CNN_block(pool_14,16*filters)\n",
    "    pool_7 = MaxPooling2D(pool_size=(2, 2))(conv_14)\n",
    "# Conlovlution and feature concatenation\n",
    "    conv_7 = Residual_CNN_block(pool_7,32*filters)\n",
    "# Upsampling with convolution \n",
    "    up_14 = attention_up_and_concatenate([conv_7, conv_14]) \n",
    "    up_conv_14 = Residual_CNN_block(up_14,16*filters)\n",
    "# Upsampling with convolution 2\n",
    "    up_28 = attention_up_and_concatenate([up_conv_14, conv_28])\n",
    "    up_conv_28 = Residual_CNN_block(up_28,8*filters)\n",
    "# Upsampling with convolution 3\n",
    "    up_56 = attention_up_and_concatenate2([up_conv_28, conv_56])\n",
    "    up_conv_56 = Residual_CNN_block(up_56,4*filters)\n",
    "# Upsampling with convolution 4\n",
    "    up_112 = attention_up_and_concatenate2([up_conv_56, conv_112])\n",
    "    up_conv_112 = Residual_CNN_block(up_112,2*filters)\n",
    "# Upsampling with convolution 5\n",
    "    up_224 = attention_up_and_concatenate2([up_conv_112, conv_224])\n",
    "    #up_224 = attention_up_and_concatenate2(up_conv_112, conv_224)\n",
    "    up_conv_224 = Residual_CNN_block(up_224,filters,dropout = last_dropout)\n",
    "# 1 dimensional convolution and generate probabilities from Sigmoid function\n",
    "    conv_final = Conv2D(OUTPUT_MASK_CHANNELS, (1, 1))(up_conv_224)  \n",
    "    conv_final = Activation('sigmoid')(conv_final)\n",
    "# Generate model\n",
    "    model = Model(inputs, conv_final, name=\"UNET_224\")\n",
    "    return model\n",
    "\n",
    "for sample_size in range(200,600,100):\n",
    "    for round_num in range(1,11):\n",
    "        \n",
    "        name = \"\"\n",
    "        name = \"model_train_from_scratch_\"\n",
    "        name += 'No_NAIP_'\n",
    "        name += str(sample_size)+\"_\"\n",
    "        name += \"samples_r\"+str(round_num)+\"_\"\n",
    "        data_path = './samples/'+str(sample_size)+'/'+str(round_num)+'/'\n",
    "        \n",
    "        print(name)\n",
    "        root_path = './training_results/'+str(sample_size)+'/'\n",
    "        \n",
    "        if os.path.exists(root_path+name+\".h5\"):\n",
    "            print(\"exists\")\n",
    "            continue;\n",
    "        \n",
    "        \n",
    "        # read in training and validation data\n",
    "        X_train_new = np.load(data_path+'train_data.npy')\n",
    "        print(X_train_new.shape)\n",
    "        Y_train = np.load(data_path+'train_label.npy')\n",
    "        print(Y_train.shape)\n",
    "        X_Validation_new = np.load(data_path+'vali_data.npy')\n",
    "        print(X_Validation_new.shape)\n",
    "        Y_Validation = np.load(data_path+'vali_label.npy')\n",
    "        print(Y_Validation.shape)\n",
    "\n",
    "\n",
    "        patch_size = 224\n",
    "        IMG_WIDTH = patch_size\n",
    "        IMG_HEIGHT = patch_size\n",
    "        # Number of feature channels \n",
    "        INPUT_CHANNELS = 8\n",
    "        # Number of output masks (1 in case you predict only one type of objects)\n",
    "        OUTPUT_MASK_CHANNELS = 1\n",
    "        maxepoch = 50\n",
    "        # hyperparameters\n",
    "        # learning_rate = 0.0000359\n",
    "        learning_rate = 0.0001\n",
    "        patience = 20\n",
    "        model = UNET_224()\n",
    "        model.compile(optimizer=Adam(lr=learning_rate),loss = dice_coef_loss,metrics=[dice_coef,'accuracy'])\n",
    "        callbacks = [\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=patience, min_lr=1e-9, verbose=1, mode='min'),\n",
    "                EarlyStopping(monitor='val_loss', patience=patience+10, verbose=0),\n",
    "                ModelCheckpoint('model_train_from_scratch_no_NAIP.h5', monitor='val_loss', save_best_only=True, verbose=0),\n",
    "            ]\n",
    "\n",
    "        # name += str(maxepoch)+'_epoch_'\n",
    "        print(name)\n",
    "\n",
    "        no_transfer_learning_results = model.fit(X_train_new, Y_train, validation_data=(X_Validation_new,Y_Validation), batch_size=2, epochs=maxepoch, callbacks=callbacks)\n",
    "\n",
    "        import pickle\n",
    "#         import time\n",
    "#         timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "#         name += timestr\n",
    "        root_path = './training_results/'+str(sample_size)+'/'\n",
    "\n",
    "        # save the trained model\n",
    "        if not os.path.exists(root_path):\n",
    "                os.makedirs(root_path)\n",
    "\n",
    "        model_yaml = model.to_yaml()\n",
    "        with open(root_path+name+\".yaml\", \"w\") as yaml_file:\n",
    "            yaml_file.write(model_yaml)\n",
    "        # save the weights\n",
    "        model.save(root_path+name+\".h5\")\n",
    "        # save the intermdediate results and training statistics\n",
    "        with open(root_path+name+\".pickle\", 'wb') as file_pi:\n",
    "            pickle.dump(no_transfer_learning_results.history, file_pi, protocol=2)\n",
    "\n",
    "from IPython.display import Audio\n",
    "sound_file = '../../wink.mp3'\n",
    "\n",
    "Audio(sound_file, autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
